{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run on TensorFlow 2.x\n",
    "%tensorflow_version 2.x\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import relevant modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "from mlp_sparse_model import MLPSparseModel\n",
    "from mlp_plain_model import MLPPlainModel\n",
    "import time\n",
    "\n",
    "# The following lines adjust the granularity of reporting. \n",
    "pd.options.display.max_rows = 10\n",
    "\n",
    "# The following line improves formatting when ouputting NumPy arrays.\n",
    "np.set_printoptions(linewidth = 200, precision=3, suppress=True)\n",
    "\n",
    "SAMPLE_SIZE = 9\n",
    "N_EXP = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_generator():\n",
    "    # Generate the initial seed for each sample size (to match the seed\n",
    "    # of the results in the paper)\n",
    "    # This is just the initial seed, for each experiment, the seeds will be\n",
    "    # equal the initial seed + the number of the experiment\n",
    "\n",
    "    N_train_all = np.multiply(9, [1, 2, 3, 4, 5])  # This is for Apache\n",
    "    if SAMPLE_SIZE in N_train_all:\n",
    "        seed_o = np.where(N_train_all == SAMPLE_SIZE)[0][0]\n",
    "    else:\n",
    "        seed_o = np.random.randint(1, 101)\n",
    "\n",
    "    return seed_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Get data\n",
    "fm_dataset = pd.read_csv(\"Data/Apache_AllNumeric.csv\")\n",
    "column_dict = {name: \"float64\" for name in list(fm_dataset.columns.values)}\n",
    "fm_dataset = fm_dataset.astype(column_dict)\n",
    "fm_dataset = fm_dataset.reindex(np.random.permutation(fm_dataset.index))\n",
    "\n",
    "fm_features = fm_dataset.copy()\n",
    "fm_labels = fm_features.pop('PERF') / 1000\n",
    "\n",
    "# fm_features = np.array(fm_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.convert_to_tensor(fm_features)\n",
    "normalizer = layers.Normalization(axis=-1)\n",
    "normalizer.adapt(fm_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basic_model():\n",
    "    model = keras.Sequential([\n",
    "        normalizer,\n",
    "        layers.Dense(20, activation='relu', kernel_regularizer=keras.regularizers.l1(0.009)),\n",
    "        layers.Dense(10, activation='relu', kernel_regularizer=keras.regularizers.l1(0.009)),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[keras.metrics.MeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 2ms/step - loss: 3.0416 - mean_squared_error: 2.3551\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 2.3360 - mean_squared_error: 1.6590\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.8234 - mean_squared_error: 1.1543\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.4422 - mean_squared_error: 0.7803\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.1637 - mean_squared_error: 0.5090\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.9702 - mean_squared_error: 0.3224\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.8511 - mean_squared_error: 0.2112\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.7912 - mean_squared_error: 0.1601\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.7552 - mean_squared_error: 0.1340\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.7257 - mean_squared_error: 0.1151\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6990 - mean_squared_error: 0.0988\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6743 - mean_squared_error: 0.0846\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6534 - mean_squared_error: 0.0740\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6355 - mean_squared_error: 0.0665\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6166 - mean_squared_error: 0.0581\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6002 - mean_squared_error: 0.0522\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5844 - mean_squared_error: 0.0469\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5693 - mean_squared_error: 0.0426\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5556 - mean_squared_error: 0.0393\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5422 - mean_squared_error: 0.0363\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5292 - mean_squared_error: 0.0337\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5166 - mean_squared_error: 0.0312\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5046 - mean_squared_error: 0.0291\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4931 - mean_squared_error: 0.0271\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4821 - mean_squared_error: 0.0256\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4716 - mean_squared_error: 0.0242\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4611 - mean_squared_error: 0.0229\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4510 - mean_squared_error: 0.0221\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4406 - mean_squared_error: 0.0207\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4306 - mean_squared_error: 0.0197\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4206 - mean_squared_error: 0.0189\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4109 - mean_squared_error: 0.0182\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4010 - mean_squared_error: 0.0174\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3915 - mean_squared_error: 0.0168\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3826 - mean_squared_error: 0.0164\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3740 - mean_squared_error: 0.0159\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3654 - mean_squared_error: 0.0157\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3569 - mean_squared_error: 0.0154\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3485 - mean_squared_error: 0.0151\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3401 - mean_squared_error: 0.0148\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3317 - mean_squared_error: 0.0145\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3234 - mean_squared_error: 0.0142\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3150 - mean_squared_error: 0.0139\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3071 - mean_squared_error: 0.0138\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2994 - mean_squared_error: 0.0136\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2919 - mean_squared_error: 0.0134\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2845 - mean_squared_error: 0.0133\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2770 - mean_squared_error: 0.0128\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2698 - mean_squared_error: 0.0126\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2628 - mean_squared_error: 0.0123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcea43aaa50>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_basic_model()\n",
    "model.fit(fm_features, fm_labels, epochs=50, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize values\n",
    "# data_df_mean = data_df.mean()\n",
    "# data_df_std = data_df.std()\n",
    "# data_df_norm = (data_df - data_df_mean)/data_df_std\n",
    "# normalize = layers.Normalization()\n",
    "# normalize.adapt(fm_features)\n",
    "# normalized_data = normalize(fm_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data set and set seed\n",
    "seed_init = seed_generator()\n",
    "seed = seed_init*N_EXP + 1\n",
    "np.random.seed(seed_init)\n",
    "train_data = fm_features.sample(frac=0.6)\n",
    "test_data = fm_features.drop(train_data.index).sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature layer\n",
    "columns = [column for column in column_dict.keys() if column != 'PERF']\n",
    "feature_columns = []\n",
    "for column in columns:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(column))\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define the plotting function.\n",
    "\n",
    "def plot_the_loss_curve(epochs, mse):\n",
    "  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Mean Squared Error\")\n",
    "\n",
    "  plt.plot(epochs, mse, label=\"Loss\")\n",
    "  plt.legend()\n",
    "  plt.ylim([mse.min()*0.95, mse.max() * 1.03])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Double-click for a possible solution\n",
    "\n",
    "# The following \"solution\" uses L2 regularization to bring training loss\n",
    "# and test loss closer to each other. Many, many other solutions are possible.\n",
    "\n",
    "\n",
    "def create_model(learning_rate, features):\n",
    "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
    "\n",
    "  normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "  normalizer.adapt(features)\n",
    "\n",
    "\n",
    "  # Discard any pre-existing version of the model.\n",
    "  model = None\n",
    "\n",
    "  # Most simple tf.keras models are sequential.\n",
    "  model = tf.keras.models.Sequential()\n",
    "\n",
    "  # Add the layer containing the feature columns to the model.\n",
    "  model.add(normalizer)\n",
    "\n",
    "  # Describe the topography of the model. \n",
    "\n",
    "  # Implement L1 regularization in the first hidden layer.\n",
    "  model.add(tf.keras.layers.Dense(units=20, \n",
    "                                  activation='relu',\n",
    "                                  kernel_regularizer=tf.keras.regularizers.l1(0.009),\n",
    "                                  name='Hidden1'))\n",
    "  \n",
    "  # Implement L1 regularization in the second hidden layer.\n",
    "  model.add(tf.keras.layers.Dense(units=12, \n",
    "                                  activation='relu', \n",
    "                                  # kernel_regularizer=tf.keras.regularizers.l1(0.009),\n",
    "                                  name='Hidden2'))\n",
    "\n",
    "  # Define the output layer.\n",
    "  model.add(tf.keras.layers.Dense(units=1,  \n",
    "                                  name='Output'))                              \n",
    "  \n",
    "  # input_layer = tf.keras.layers.Input(shape=(1))\n",
    "\n",
    "  # layer_1 = tf.keras.layers.Dense(units=20,\n",
    "  #                                 activation='relu',\n",
    "  #                                 kernel_regularizer=tf.keras.regularizers.l1(0.009),\n",
    "  #                                 name='Hidden1')(input_layer)\n",
    "  # layer_2 = tf.keras.layers.Dense(units=12,\n",
    "  #                                 activation='relu',\n",
    "  #                                 # kernel_regularizer=tf.keras.regularizers.l1(0.009),\n",
    "  #                                 name='Hidden2')(layer_1)\n",
    "\n",
    "  # output_layer = tf.keras.layers.Dense(units=1,\n",
    "  #                                      name='Output')(layer_2)\n",
    "\n",
    "  # model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "  model.compile(optimizer=keras.optimizers.Adam(lr=learning_rate),\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[keras.metrics.MeanSquaredError()])\n",
    "\n",
    "  return model     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_20 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " Hidden1 (Dense)             (None, 20)                40        \n",
      "                                                                 \n",
      " Hidden2 (Dense)             (None, 12)                252       \n",
      "                                                                 \n",
      " Output (Dense)              (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 305\n",
      "Trainable params: 305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = tf.keras.layers.Input(shape=(1))\n",
    "display(input_layer.shape)\n",
    "layer_1 = tf.keras.layers.Dense(units=20,\n",
    "                                activation='relu',\n",
    "                                kernel_regularizer=tf.keras.regularizers.l1(0.009),\n",
    "                                name='Hidden1')(input_layer)\n",
    "layer_2 = tf.keras.layers.Dense(units=12,\n",
    "                                activation='relu',\n",
    "                                # kernel_regularizer=tf.keras.regularizers.l1(0.009),\n",
    "                                name='Hidden2')(layer_1)\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(units=1,\n",
    "                                    name='Output')(layer_2)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, epochs, label,\n",
    "                batch_size=None, validation_split=0.1):\n",
    "  \"\"\"Train the model by feeding it data.\"\"\"\n",
    "\n",
    "  # Split the dataset into features and label.\n",
    "  features = {name:np.array(value) for name, value in dataset.items()}\n",
    "#   label = np.array(features.pop(label_name))\n",
    "  history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=True, validation_split=validation_split) \n",
    "\n",
    "  # The list of epochs is stored separately from the rest of history.\n",
    "  epochs = history.epoch\n",
    "  \n",
    "  # To track the progression of training, gather a snapshot\n",
    "  # of the model's mean squared error at each epoch. \n",
    "  hist = pd.DataFrame(history.history)\n",
    "  mse = hist[\"mean_squared_error\"]\n",
    "\n",
    "  return epochs, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "add() got an unexpected keyword argument 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-2ebcb968d343>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Establish the model's topography.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmy_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfm_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# Train the model on the normalized training set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m epochs, mse = train_model(my_model, train_data, epochs, \n",
      "\u001b[0;32m<ipython-input-32-c01ceaa1cea1>\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(learning_rate, features)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;31m# Add the layer containing the feature columns to the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"normalizer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;31m# Describe the topography of the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: add() got an unexpected keyword argument 'name'"
     ]
    }
   ],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.004\n",
    "epochs = 100\n",
    "batch_size = 10\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model(learning_rate, tf.convert_to_tensor(fm_features))\n",
    "# Train the model on the normalized training set.\n",
    "epochs, mse = train_model(my_model, train_data, epochs, \n",
    "                          fm_labels, batch_size, validation_split=0.1)\n",
    "my_model.summary()\n",
    "# plot_the_loss_curve(epochs, mse)\n",
    "\n",
    "# test_features = {name:np.array(value) for name, value in test_df.items()}\n",
    "# test_label = np.array(test_features.pop(label_name)) # isolate the label\n",
    "# print(\"\\n Evaluate the new model against the test set:\")\n",
    "# my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size) \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
