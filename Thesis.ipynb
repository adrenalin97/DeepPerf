{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run on TensorFlow 2.x\n",
    "%tensorflow_version 2.x\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import relevant modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "from mlp_sparse_model import MLPSparseModel\n",
    "from mlp_plain_model import MLPPlainModel\n",
    "import time\n",
    "\n",
    "# The following lines adjust the granularity of reporting. \n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.7f}\".format\n",
    "\n",
    "# The following line improves formatting when ouputting NumPy arrays.\n",
    "np.set_printoptions(linewidth = 200)\n",
    "\n",
    "SAMPLE_SIZE = 11\n",
    "N_EXP = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_generator():\n",
    "    # Generate the initial seed for each sample size (to match the seed\n",
    "    # of the results in the paper)\n",
    "    # This is just the initial seed, for each experiment, the seeds will be\n",
    "    # equal the initial seed + the number of the experiment\n",
    "\n",
    "    N_train_all = np.multiply(11, [1, 2, 3, 4, 5])  # This is for LLVM\n",
    "    if SAMPLE_SIZE in N_train_all:\n",
    "        seed_o = np.where(N_train_all == SAMPLE_SIZE)[0][0]\n",
    "    else:\n",
    "        seed_o = np.random.randint(1, 101)\n",
    "\n",
    "    return seed_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Get data\n",
    "data_df = pd.read_csv(\"Data/LLVM_AllNumeric.csv\")\n",
    "column_dict = {name: bool for name in list(data_df.columns.values) if name != 'PERF'}\n",
    "column_dict['PERF'] = \"float64\"\n",
    "data_df = data_df.astype(column_dict)\n",
    "data_df = data_df.reindex(np.random.permutation(data_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize values\n",
    "data_df_mean = data_df.mean()\n",
    "data_df_std = data_df.std()\n",
    "data_df_norm = (data_df - data_df_mean)/data_df_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data set and set seed\n",
    "seed_init = seed_generator()\n",
    "seed = seed_init*N_EXP + 1\n",
    "np.random.seed(seed_init)\n",
    "train_df = data_df_norm.sample(frac=0.6)\n",
    "test_df = data_df_norm.drop(train_df.index).sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature layer\n",
    "columns = [column for column in column_dict.keys() if column != 'PERF']\n",
    "feature_columns = []\n",
    "for column in columns:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(column))\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define the plotting function.\n",
    "\n",
    "def plot_the_loss_curve(epochs, mse):\n",
    "  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Mean Squared Error\")\n",
    "\n",
    "  plt.plot(epochs, mse, label=\"Loss\")\n",
    "  plt.legend()\n",
    "  plt.ylim([mse.min()*0.95, mse.max() * 1.03])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Double-click for a possible solution\n",
    "\n",
    "# The following \"solution\" uses L2 regularization to bring training loss\n",
    "# and test loss closer to each other. Many, many other solutions are possible.\n",
    "\n",
    "\n",
    "def create_model(learning_rate, feature_layer):\n",
    "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
    "\n",
    "  # Discard any pre-existing version of the model.\n",
    "  model = None\n",
    "\n",
    "  # Most simple tf.keras models are sequential.\n",
    "  model = tf.keras.models.Sequential()\n",
    "\n",
    "  # Add the layer containing the feature columns to the model.\n",
    "  model.add(feature_layer)\n",
    "\n",
    "  # Describe the topography of the model. \n",
    "\n",
    "  # Implement L2 regularization in the first hidden layer.\n",
    "  model.add(tf.keras.layers.Dense(units=20, \n",
    "                                  activation='relu',\n",
    "                                  kernel_regularizer=tf.keras.regularizers.l1(0.009),\n",
    "                                  name='Hidden1'))\n",
    "  \n",
    "  # Implement L2 regularization in the second hidden layer.\n",
    "  model.add(tf.keras.layers.Dense(units=12, \n",
    "                                  activation='relu', \n",
    "                                  kernel_regularizer=tf.keras.regularizers.l1(0.009),\n",
    "                                  name='Hidden2'))\n",
    "\n",
    "  # Define the output layer.\n",
    "  model.add(tf.keras.layers.Dense(units=1,  \n",
    "                                  name='Output'))                              \n",
    "  \n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "  return model     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, epochs, label_name,\n",
    "                batch_size=None, validation_split=0.1):\n",
    "  \"\"\"Train the model by feeding it data.\"\"\"\n",
    "\n",
    "  # Split the dataset into features and label.\n",
    "  features = {name:np.array(value) for name, value in dataset.items()}\n",
    "  label = np.array(features.pop(label_name))\n",
    "  history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=True, validation_split=validation_split) \n",
    "\n",
    "  # The list of epochs is stored separately from the rest of history.\n",
    "  epochs = history.epoch\n",
    "  \n",
    "  # To track the progression of training, gather a snapshot\n",
    "  # of the model's mean squared error at each epoch. \n",
    "  hist = pd.DataFrame(history.history)\n",
    "  mse = hist[\"mean_squared_error\"]\n",
    "\n",
    "  return epochs, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'gvn': <tf.Tensor 'IteratorGetNext:0' shape=(None,) dtype=float32>, 'instcombine': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=float32>, 'inline': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=float32>, 'jump_threading': <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=float32>, 'simplifycfg': <tf.Tensor 'IteratorGetNext:9' shape=(None,) dtype=float32>, 'sccp': <tf.Tensor 'IteratorGetNext:8' shape=(None,) dtype=float32>, 'print_used_types': <tf.Tensor 'IteratorGetNext:7' shape=(None,) dtype=float32>, 'ipsccp': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=float32>, 'iv_users': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=float32>, 'licm': <tf.Tensor 'IteratorGetNext:6' shape=(None,) dtype=float32>}. Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'gvn': <tf.Tensor 'IteratorGetNext:0' shape=(None,) dtype=float32>, 'instcombine': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=float32>, 'inline': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=float32>, 'jump_threading': <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=float32>, 'simplifycfg': <tf.Tensor 'IteratorGetNext:9' shape=(None,) dtype=float32>, 'sccp': <tf.Tensor 'IteratorGetNext:8' shape=(None,) dtype=float32>, 'print_used_types': <tf.Tensor 'IteratorGetNext:7' shape=(None,) dtype=float32>, 'ipsccp': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=float32>, 'iv_users': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=float32>, 'licm': <tf.Tensor 'IteratorGetNext:6' shape=(None,) dtype=float32>}. Consider rewriting this model with the Functional API.\n",
      "52/56 [==========================>...] - ETA: 0s - loss: 1.5011 - mean_squared_error: 0.7186WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'gvn': <tf.Tensor 'IteratorGetNext:0' shape=(None,) dtype=float32>, 'instcombine': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=float32>, 'inline': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=float32>, 'jump_threading': <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=float32>, 'simplifycfg': <tf.Tensor 'IteratorGetNext:9' shape=(None,) dtype=float32>, 'sccp': <tf.Tensor 'IteratorGetNext:8' shape=(None,) dtype=float32>, 'print_used_types': <tf.Tensor 'IteratorGetNext:7' shape=(None,) dtype=float32>, 'ipsccp': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=float32>, 'iv_users': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=float32>, 'licm': <tf.Tensor 'IteratorGetNext:6' shape=(None,) dtype=float32>}. Consider rewriting this model with the Functional API.\n",
      "56/56 [==============================] - 2s 9ms/step - loss: 1.4813 - mean_squared_error: 0.7028 - val_loss: 1.1061 - val_mean_squared_error: 0.4018\n",
      "Epoch 2/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.8874 - mean_squared_error: 0.2492 - val_loss: 0.7899 - val_mean_squared_error: 0.2210\n",
      "Epoch 3/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.6691 - mean_squared_error: 0.1583 - val_loss: 0.6327 - val_mean_squared_error: 0.1818\n",
      "Epoch 4/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.5289 - mean_squared_error: 0.1232 - val_loss: 0.5101 - val_mean_squared_error: 0.1473\n",
      "Epoch 5/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.4341 - mean_squared_error: 0.1029 - val_loss: 0.4354 - val_mean_squared_error: 0.1369\n",
      "Epoch 6/100\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 0.3707 - mean_squared_error: 0.0969 - val_loss: 0.3736 - val_mean_squared_error: 0.1236\n",
      "Epoch 7/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.3244 - mean_squared_error: 0.0941 - val_loss: 0.3259 - val_mean_squared_error: 0.1147\n",
      "Epoch 8/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.2894 - mean_squared_error: 0.0921 - val_loss: 0.2998 - val_mean_squared_error: 0.1147\n",
      "Epoch 9/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.2660 - mean_squared_error: 0.0908 - val_loss: 0.2844 - val_mean_squared_error: 0.1145\n",
      "Epoch 10/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.2494 - mean_squared_error: 0.0897 - val_loss: 0.2618 - val_mean_squared_error: 0.1097\n",
      "Epoch 11/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.2403 - mean_squared_error: 0.0902 - val_loss: 0.2489 - val_mean_squared_error: 0.1068\n",
      "Epoch 12/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.2299 - mean_squared_error: 0.0907 - val_loss: 0.2422 - val_mean_squared_error: 0.1088\n",
      "Epoch 13/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.2213 - mean_squared_error: 0.0903 - val_loss: 0.2350 - val_mean_squared_error: 0.1067\n",
      "Epoch 14/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.2166 - mean_squared_error: 0.0903 - val_loss: 0.2371 - val_mean_squared_error: 0.1124\n",
      "Epoch 15/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.2197 - mean_squared_error: 0.0947 - val_loss: 0.2312 - val_mean_squared_error: 0.1114\n",
      "Epoch 16/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.2076 - mean_squared_error: 0.0905 - val_loss: 0.2157 - val_mean_squared_error: 0.1002\n",
      "Epoch 17/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.2006 - mean_squared_error: 0.0879 - val_loss: 0.2134 - val_mean_squared_error: 0.1018\n",
      "Epoch 18/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.1924 - mean_squared_error: 0.0823 - val_loss: 0.2149 - val_mean_squared_error: 0.1059\n",
      "Epoch 19/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1928 - mean_squared_error: 0.0860 - val_loss: 0.2026 - val_mean_squared_error: 0.0957\n",
      "Epoch 20/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.1902 - mean_squared_error: 0.0849 - val_loss: 0.1985 - val_mean_squared_error: 0.0968\n",
      "Epoch 21/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1856 - mean_squared_error: 0.0836 - val_loss: 0.1992 - val_mean_squared_error: 0.0969\n",
      "Epoch 22/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1860 - mean_squared_error: 0.0852 - val_loss: 0.2053 - val_mean_squared_error: 0.1055\n",
      "Epoch 23/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1790 - mean_squared_error: 0.0817 - val_loss: 0.1873 - val_mean_squared_error: 0.0915\n",
      "Epoch 24/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.1762 - mean_squared_error: 0.0818 - val_loss: 0.1857 - val_mean_squared_error: 0.0918\n",
      "Epoch 25/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.1723 - mean_squared_error: 0.0797 - val_loss: 0.1805 - val_mean_squared_error: 0.0885\n",
      "Epoch 26/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.1708 - mean_squared_error: 0.0806 - val_loss: 0.1779 - val_mean_squared_error: 0.0892\n",
      "Epoch 27/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1703 - mean_squared_error: 0.0815 - val_loss: 0.1943 - val_mean_squared_error: 0.1054\n",
      "Epoch 28/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.2028 - mean_squared_error: 0.1101 - val_loss: 0.1712 - val_mean_squared_error: 0.0829\n",
      "Epoch 29/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1646 - mean_squared_error: 0.0769 - val_loss: 0.1719 - val_mean_squared_error: 0.0853\n",
      "Epoch 30/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1659 - mean_squared_error: 0.0785 - val_loss: 0.1720 - val_mean_squared_error: 0.0862\n",
      "Epoch 31/100\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 0.1699 - mean_squared_error: 0.0835 - val_loss: 0.1734 - val_mean_squared_error: 0.0888\n",
      "Epoch 32/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1613 - mean_squared_error: 0.0764 - val_loss: 0.1649 - val_mean_squared_error: 0.0818\n",
      "Epoch 33/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1673 - mean_squared_error: 0.0812 - val_loss: 0.1695 - val_mean_squared_error: 0.0834\n",
      "Epoch 34/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1628 - mean_squared_error: 0.0779 - val_loss: 0.1619 - val_mean_squared_error: 0.0803\n",
      "Epoch 35/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1546 - mean_squared_error: 0.0732 - val_loss: 0.1552 - val_mean_squared_error: 0.0749\n",
      "Epoch 36/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.1535 - mean_squared_error: 0.0736 - val_loss: 0.1518 - val_mean_squared_error: 0.0718\n",
      "Epoch 37/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1510 - mean_squared_error: 0.0716 - val_loss: 0.1674 - val_mean_squared_error: 0.0891\n",
      "Epoch 38/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.1563 - mean_squared_error: 0.0774 - val_loss: 0.1615 - val_mean_squared_error: 0.0843\n",
      "Epoch 39/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1504 - mean_squared_error: 0.0735 - val_loss: 0.1598 - val_mean_squared_error: 0.0832\n",
      "Epoch 40/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1492 - mean_squared_error: 0.0737 - val_loss: 0.1495 - val_mean_squared_error: 0.0739\n",
      "Epoch 41/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.1471 - mean_squared_error: 0.0726 - val_loss: 0.1434 - val_mean_squared_error: 0.0689\n",
      "Epoch 42/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1468 - mean_squared_error: 0.0727 - val_loss: 0.1434 - val_mean_squared_error: 0.0703\n",
      "Epoch 43/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1409 - mean_squared_error: 0.0685 - val_loss: 0.1428 - val_mean_squared_error: 0.0706\n",
      "Epoch 44/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.1401 - mean_squared_error: 0.0691 - val_loss: 0.1396 - val_mean_squared_error: 0.0692\n",
      "Epoch 45/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1458 - mean_squared_error: 0.0746 - val_loss: 0.1399 - val_mean_squared_error: 0.0687\n",
      "Epoch 46/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.1446 - mean_squared_error: 0.0743 - val_loss: 0.1531 - val_mean_squared_error: 0.0830\n",
      "Epoch 47/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1439 - mean_squared_error: 0.0733 - val_loss: 0.1379 - val_mean_squared_error: 0.0685\n",
      "Epoch 48/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1414 - mean_squared_error: 0.0714 - val_loss: 0.1392 - val_mean_squared_error: 0.0693\n",
      "Epoch 49/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1387 - mean_squared_error: 0.0693 - val_loss: 0.1392 - val_mean_squared_error: 0.0707\n",
      "Epoch 50/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1396 - mean_squared_error: 0.0709 - val_loss: 0.1377 - val_mean_squared_error: 0.0692\n",
      "Epoch 51/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1415 - mean_squared_error: 0.0732 - val_loss: 0.1385 - val_mean_squared_error: 0.0703\n",
      "Epoch 52/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1385 - mean_squared_error: 0.0706 - val_loss: 0.1372 - val_mean_squared_error: 0.0699\n",
      "Epoch 53/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1353 - mean_squared_error: 0.0679 - val_loss: 0.1389 - val_mean_squared_error: 0.0719\n",
      "Epoch 54/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.1383 - mean_squared_error: 0.0708 - val_loss: 0.1425 - val_mean_squared_error: 0.0766\n",
      "Epoch 55/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1394 - mean_squared_error: 0.0729 - val_loss: 0.1368 - val_mean_squared_error: 0.0711\n",
      "Epoch 56/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.1400 - mean_squared_error: 0.0730 - val_loss: 0.1391 - val_mean_squared_error: 0.0729\n",
      "Epoch 57/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.1470 - mean_squared_error: 0.0807 - val_loss: 0.1401 - val_mean_squared_error: 0.0754\n",
      "Epoch 58/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1349 - mean_squared_error: 0.0692 - val_loss: 0.1515 - val_mean_squared_error: 0.0871\n",
      "Epoch 59/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1388 - mean_squared_error: 0.0739 - val_loss: 0.1392 - val_mean_squared_error: 0.0765\n",
      "Epoch 60/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1385 - mean_squared_error: 0.0748 - val_loss: 0.1330 - val_mean_squared_error: 0.0697\n",
      "Epoch 61/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.1352 - mean_squared_error: 0.0712 - val_loss: 0.1348 - val_mean_squared_error: 0.0718\n",
      "Epoch 62/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.1355 - mean_squared_error: 0.0733 - val_loss: 0.1319 - val_mean_squared_error: 0.0698\n",
      "Epoch 63/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1319 - mean_squared_error: 0.0696 - val_loss: 0.1353 - val_mean_squared_error: 0.0726\n",
      "Epoch 64/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.1311 - mean_squared_error: 0.0691 - val_loss: 0.1360 - val_mean_squared_error: 0.0739\n",
      "Epoch 65/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1319 - mean_squared_error: 0.0703 - val_loss: 0.1294 - val_mean_squared_error: 0.0671\n",
      "Epoch 66/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.1286 - mean_squared_error: 0.0676 - val_loss: 0.1360 - val_mean_squared_error: 0.0750\n",
      "Epoch 67/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1345 - mean_squared_error: 0.0736 - val_loss: 0.1301 - val_mean_squared_error: 0.0693\n",
      "Epoch 68/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1307 - mean_squared_error: 0.0704 - val_loss: 0.1290 - val_mean_squared_error: 0.0680\n",
      "Epoch 69/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.1304 - mean_squared_error: 0.0703 - val_loss: 0.1308 - val_mean_squared_error: 0.0712\n",
      "Epoch 70/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.1338 - mean_squared_error: 0.0743 - val_loss: 0.1406 - val_mean_squared_error: 0.0803\n",
      "Epoch 71/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.1327 - mean_squared_error: 0.0726 - val_loss: 0.1365 - val_mean_squared_error: 0.0768\n",
      "Epoch 72/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1366 - mean_squared_error: 0.0754 - val_loss: 0.1299 - val_mean_squared_error: 0.0696\n",
      "Epoch 73/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.1292 - mean_squared_error: 0.0703 - val_loss: 0.1281 - val_mean_squared_error: 0.0696\n",
      "Epoch 74/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1331 - mean_squared_error: 0.0742 - val_loss: 0.1408 - val_mean_squared_error: 0.0833\n",
      "Epoch 75/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.1291 - mean_squared_error: 0.0719 - val_loss: 0.1308 - val_mean_squared_error: 0.0723\n",
      "Epoch 76/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1289 - mean_squared_error: 0.0704 - val_loss: 0.1253 - val_mean_squared_error: 0.0670\n",
      "Epoch 77/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.1273 - mean_squared_error: 0.0697 - val_loss: 0.1244 - val_mean_squared_error: 0.0664\n",
      "Epoch 78/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1274 - mean_squared_error: 0.0701 - val_loss: 0.1329 - val_mean_squared_error: 0.0767\n",
      "Epoch 79/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.1275 - mean_squared_error: 0.0710 - val_loss: 0.1356 - val_mean_squared_error: 0.0794\n",
      "Epoch 80/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.1300 - mean_squared_error: 0.0729 - val_loss: 0.1275 - val_mean_squared_error: 0.0711\n",
      "Epoch 81/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.1310 - mean_squared_error: 0.0743 - val_loss: 0.1267 - val_mean_squared_error: 0.0706\n",
      "Epoch 82/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1236 - mean_squared_error: 0.0675 - val_loss: 0.1386 - val_mean_squared_error: 0.0820\n",
      "Epoch 83/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1248 - mean_squared_error: 0.0685 - val_loss: 0.1385 - val_mean_squared_error: 0.0817\n",
      "Epoch 84/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.1252 - mean_squared_error: 0.0693 - val_loss: 0.1345 - val_mean_squared_error: 0.0790\n",
      "Epoch 85/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.1262 - mean_squared_error: 0.0701 - val_loss: 0.1283 - val_mean_squared_error: 0.0723\n",
      "Epoch 86/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.1244 - mean_squared_error: 0.0687 - val_loss: 0.1239 - val_mean_squared_error: 0.0676\n",
      "Epoch 87/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.1332 - mean_squared_error: 0.0766 - val_loss: 0.1251 - val_mean_squared_error: 0.0694\n",
      "Epoch 88/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.1250 - mean_squared_error: 0.0691 - val_loss: 0.1230 - val_mean_squared_error: 0.0671\n",
      "Epoch 89/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.1277 - mean_squared_error: 0.0712 - val_loss: 0.1188 - val_mean_squared_error: 0.0630\n",
      "Epoch 90/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.1228 - mean_squared_error: 0.0671 - val_loss: 0.1205 - val_mean_squared_error: 0.0636\n",
      "Epoch 91/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1220 - mean_squared_error: 0.0658 - val_loss: 0.1275 - val_mean_squared_error: 0.0713\n",
      "Epoch 92/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.1231 - mean_squared_error: 0.0675 - val_loss: 0.1242 - val_mean_squared_error: 0.0687\n",
      "Epoch 93/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1216 - mean_squared_error: 0.0656 - val_loss: 0.1308 - val_mean_squared_error: 0.0759\n",
      "Epoch 94/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.1223 - mean_squared_error: 0.0666 - val_loss: 0.1269 - val_mean_squared_error: 0.0713\n",
      "Epoch 95/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.1211 - mean_squared_error: 0.0654 - val_loss: 0.1215 - val_mean_squared_error: 0.0657\n",
      "Epoch 96/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1267 - mean_squared_error: 0.0711 - val_loss: 0.1196 - val_mean_squared_error: 0.0634\n",
      "Epoch 97/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1211 - mean_squared_error: 0.0657 - val_loss: 0.1238 - val_mean_squared_error: 0.0682\n",
      "Epoch 98/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1221 - mean_squared_error: 0.0659 - val_loss: 0.1236 - val_mean_squared_error: 0.0678\n",
      "Epoch 99/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.1218 - mean_squared_error: 0.0662 - val_loss: 0.1216 - val_mean_squared_error: 0.0662\n",
      "Epoch 100/100\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1232 - mean_squared_error: 0.0676 - val_loss: 0.1254 - val_mean_squared_error: 0.0705\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluate the new model against the test set:\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'gvn': <tf.Tensor 'IteratorGetNext:0' shape=(10,) dtype=float32>, 'instcombine': <tf.Tensor 'IteratorGetNext:2' shape=(10,) dtype=float32>, 'inline': <tf.Tensor 'IteratorGetNext:1' shape=(10,) dtype=float32>, 'jump_threading': <tf.Tensor 'IteratorGetNext:5' shape=(10,) dtype=float32>, 'simplifycfg': <tf.Tensor 'IteratorGetNext:9' shape=(10,) dtype=float32>, 'sccp': <tf.Tensor 'IteratorGetNext:8' shape=(10,) dtype=float32>, 'print_used_types': <tf.Tensor 'IteratorGetNext:7' shape=(10,) dtype=float32>, 'ipsccp': <tf.Tensor 'IteratorGetNext:3' shape=(10,) dtype=float32>, 'iv_users': <tf.Tensor 'IteratorGetNext:4' shape=(10,) dtype=float32>, 'licm': <tf.Tensor 'IteratorGetNext:6' shape=(10,) dtype=float32>}. Consider rewriting this model with the Functional API.\n",
      "41/41 [==============================] - 0s 4ms/step - loss: 0.1296 - mean_squared_error: 0.0747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.12964580953121185, 0.07469898462295532]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.004\n",
    "epochs = 100\n",
    "batch_size = 10\n",
    "\n",
    "label_name = \"PERF\"\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model(learning_rate, feature_layer)\n",
    "\n",
    "# Train the model on the normalized training set.\n",
    "epochs, mse = train_model(my_model, train_df, epochs, \n",
    "                          label_name, batch_size, validation_split=0.1)\n",
    "plot_the_loss_curve(epochs, mse)\n",
    "\n",
    "test_features = {name:np.array(value) for name, value in test_df.items()}\n",
    "test_label = np.array(test_features.pop(label_name)) # isolate the label\n",
    "print(\"\\n Evaluate the new model against the test set:\")\n",
    "my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
