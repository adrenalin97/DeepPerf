{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run on TensorFlow 2.x\n",
    "%tensorflow_version 2.x\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import relevant modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "from mlp_sparse_model import MLPSparseModel\n",
    "from mlp_plain_model import MLPPlainModel\n",
    "import time\n",
    "\n",
    "# The following lines adjust the granularity of reporting. \n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.7f}\".format\n",
    "\n",
    "# The following line improves formatting when ouputting NumPy arrays.\n",
    "np.set_printoptions(linewidth = 200)\n",
    "\n",
    "SAMPLE_SIZE = 11\n",
    "N_EXP = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_generator():\n",
    "    # Generate the initial seed for each sample size (to match the seed\n",
    "    # of the results in the paper)\n",
    "    # This is just the initial seed, for each experiment, the seeds will be\n",
    "    # equal the initial seed + the number of the experiment\n",
    "\n",
    "    N_train_all = np.multiply(11, [1, 2, 3, 4, 5])  # This is for LLVM\n",
    "    if SAMPLE_SIZE in N_train_all:\n",
    "        seed_o = np.where(N_train_all == SAMPLE_SIZE)[0][0]\n",
    "    else:\n",
    "        seed_o = np.random.randint(1, 101)\n",
    "\n",
    "    return seed_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Get data\n",
    "data_df = pd.read_csv(\"Data/LLVM_AllNumeric.csv\")\n",
    "column_dict = {name: bool for name in list(data_df.columns.values) if name != 'PERF'}\n",
    "column_dict['PERF'] = \"float64\"\n",
    "data_df = data_df.astype(column_dict)\n",
    "data_df = data_df.reindex(np.random.permutation(data_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize values\n",
    "data_df_mean = data_df.mean()\n",
    "data_df_std = data_df.std()\n",
    "data_df_norm = (data_df - data_df_mean)/data_df_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data set and set seed\n",
    "seed_init = seed_generator()\n",
    "seed = seed_init*N_EXP + 1\n",
    "np.random.seed(seed_init)\n",
    "train_df = data_df_norm.sample(frac=0.6)\n",
    "test_df = data_df_norm.drop(train_df.index).sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature layer\n",
    "columns = [column for column in column_dict.keys() if column != 'PERF']\n",
    "feature_columns = []\n",
    "for column in columns:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(column))\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define the plotting function.\n",
    "\n",
    "def plot_the_loss_curve(epochs, mse):\n",
    "  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Mean Squared Error\")\n",
    "\n",
    "  plt.plot(epochs, mse, label=\"Loss\")\n",
    "  plt.legend()\n",
    "  plt.ylim([mse.min()*0.95, mse.max() * 1.03])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Double-click for a possible solution\n",
    "\n",
    "# The following \"solution\" uses L2 regularization to bring training loss\n",
    "# and test loss closer to each other. Many, many other solutions are possible.\n",
    "\n",
    "\n",
    "def create_model(learning_rate, feature_layer):\n",
    "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
    "\n",
    "  # Discard any pre-existing version of the model.\n",
    "  model = None\n",
    "\n",
    "  # Most simple tf.keras models are sequential.\n",
    "  model = tf.keras.models.Sequential()\n",
    "\n",
    "  # Add the layer containing the feature columns to the model.\n",
    "  model.add(feature_layer)\n",
    "\n",
    "  # Describe the topography of the model. \n",
    "\n",
    "  # Implement L2 regularization in the first hidden layer.\n",
    "  model.add(tf.keras.layers.Dense(units=20, \n",
    "                                  activation='relu',\n",
    "                                  kernel_regularizer=tf.keras.regularizers.l1(0.001),\n",
    "                                  name='Hidden1'))\n",
    "  \n",
    "  # Implement L2 regularization in the second hidden layer.\n",
    "  model.add(tf.keras.layers.Dense(units=12, \n",
    "                                  activation='relu', \n",
    "                                  kernel_regularizer=tf.keras.regularizers.l1(0.001),\n",
    "                                  name='Hidden2'))\n",
    "\n",
    "  # Define the output layer.\n",
    "  model.add(tf.keras.layers.Dense(units=1,  \n",
    "                                  name='Output'))                              \n",
    "  \n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "  return model     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, epochs, label_name,\n",
    "                batch_size=None, validation_split=0.1):\n",
    "  \"\"\"Train the model by feeding it data.\"\"\"\n",
    "\n",
    "  # Split the dataset into features and label.\n",
    "  features = {name:np.array(value) for name, value in dataset.items()}\n",
    "  label = np.array(features.pop(label_name))\n",
    "  history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=True, validation_split=validation_split) \n",
    "\n",
    "  # The list of epochs is stored separately from the rest of history.\n",
    "  epochs = history.epoch\n",
    "  \n",
    "  # To track the progression of training, gather a snapshot\n",
    "  # of the model's mean squared error at each epoch. \n",
    "  hist = pd.DataFrame(history.history)\n",
    "  mse = hist[\"mean_squared_error\"]\n",
    "\n",
    "  return epochs, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'gvn': <tf.Tensor 'IteratorGetNext:0' shape=(None,) dtype=float32>, 'instcombine': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=float32>, 'inline': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=float32>, 'jump_threading': <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=float32>, 'simplifycfg': <tf.Tensor 'IteratorGetNext:9' shape=(None,) dtype=float32>, 'sccp': <tf.Tensor 'IteratorGetNext:8' shape=(None,) dtype=float32>, 'print_used_types': <tf.Tensor 'IteratorGetNext:7' shape=(None,) dtype=float32>, 'ipsccp': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=float32>, 'iv_users': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=float32>, 'licm': <tf.Tensor 'IteratorGetNext:6' shape=(None,) dtype=float32>}. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'gvn': <tf.Tensor 'IteratorGetNext:0' shape=(None,) dtype=float32>, 'instcombine': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=float32>, 'inline': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=float32>, 'jump_threading': <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=float32>, 'simplifycfg': <tf.Tensor 'IteratorGetNext:9' shape=(None,) dtype=float32>, 'sccp': <tf.Tensor 'IteratorGetNext:8' shape=(None,) dtype=float32>, 'print_used_types': <tf.Tensor 'IteratorGetNext:7' shape=(None,) dtype=float32>, 'ipsccp': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=float32>, 'iv_users': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=float32>, 'licm': <tf.Tensor 'IteratorGetNext:6' shape=(None,) dtype=float32>}. Consider rewriting this model with the Functional API.\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 0.6795 - mean_squared_error: 0.5806WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'gvn': <tf.Tensor 'IteratorGetNext:0' shape=(None,) dtype=float32>, 'instcombine': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=float32>, 'inline': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=float32>, 'jump_threading': <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=float32>, 'simplifycfg': <tf.Tensor 'IteratorGetNext:9' shape=(None,) dtype=float32>, 'sccp': <tf.Tensor 'IteratorGetNext:8' shape=(None,) dtype=float32>, 'print_used_types': <tf.Tensor 'IteratorGetNext:7' shape=(None,) dtype=float32>, 'ipsccp': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=float32>, 'iv_users': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=float32>, 'licm': <tf.Tensor 'IteratorGetNext:6' shape=(None,) dtype=float32>}. Consider rewriting this model with the Functional API.\n",
      "56/56 [==============================] - 1s 8ms/step - loss: 0.6687 - mean_squared_error: 0.5698 - val_loss: 0.2693 - val_mean_squared_error: 0.1735\n",
      "Epoch 2/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.2860 - mean_squared_error: 0.1931 - val_loss: 0.2268 - val_mean_squared_error: 0.1373\n",
      "Epoch 3/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.2085 - mean_squared_error: 0.1214 - val_loss: 0.1890 - val_mean_squared_error: 0.1047\n",
      "Epoch 4/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1683 - mean_squared_error: 0.0867 - val_loss: 0.1710 - val_mean_squared_error: 0.0926\n",
      "Epoch 5/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1464 - mean_squared_error: 0.0702 - val_loss: 0.1577 - val_mean_squared_error: 0.0841\n",
      "Epoch 6/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1448 - mean_squared_error: 0.0732 - val_loss: 0.1378 - val_mean_squared_error: 0.0689\n",
      "Epoch 7/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1341 - mean_squared_error: 0.0671 - val_loss: 0.1401 - val_mean_squared_error: 0.0747\n",
      "Epoch 8/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1295 - mean_squared_error: 0.0651 - val_loss: 0.1342 - val_mean_squared_error: 0.0714\n",
      "Epoch 9/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1211 - mean_squared_error: 0.0600 - val_loss: 0.1365 - val_mean_squared_error: 0.0769\n",
      "Epoch 10/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1195 - mean_squared_error: 0.0609 - val_loss: 0.1320 - val_mean_squared_error: 0.0737\n",
      "Epoch 11/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1387 - mean_squared_error: 0.0800 - val_loss: 0.1303 - val_mean_squared_error: 0.0727\n",
      "Epoch 12/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1286 - mean_squared_error: 0.0716 - val_loss: 0.1236 - val_mean_squared_error: 0.0681\n",
      "Epoch 13/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1247 - mean_squared_error: 0.0699 - val_loss: 0.1239 - val_mean_squared_error: 0.0695\n",
      "Epoch 14/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1218 - mean_squared_error: 0.0687 - val_loss: 0.1226 - val_mean_squared_error: 0.0704\n",
      "Epoch 15/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1135 - mean_squared_error: 0.0625 - val_loss: 0.1131 - val_mean_squared_error: 0.0636\n",
      "Epoch 16/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1135 - mean_squared_error: 0.0636 - val_loss: 0.1056 - val_mean_squared_error: 0.0559\n",
      "Epoch 17/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1158 - mean_squared_error: 0.0663 - val_loss: 0.1220 - val_mean_squared_error: 0.0723\n",
      "Epoch 18/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1176 - mean_squared_error: 0.0680 - val_loss: 0.1165 - val_mean_squared_error: 0.0679\n",
      "Epoch 19/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1088 - mean_squared_error: 0.0611 - val_loss: 0.1006 - val_mean_squared_error: 0.0545\n",
      "Epoch 20/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1071 - mean_squared_error: 0.0610 - val_loss: 0.1188 - val_mean_squared_error: 0.0724\n",
      "Epoch 21/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1139 - mean_squared_error: 0.0672 - val_loss: 0.1168 - val_mean_squared_error: 0.0715\n",
      "Epoch 22/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1192 - mean_squared_error: 0.0741 - val_loss: 0.1058 - val_mean_squared_error: 0.0606\n",
      "Epoch 23/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1046 - mean_squared_error: 0.0600 - val_loss: 0.1146 - val_mean_squared_error: 0.0699\n",
      "Epoch 24/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1105 - mean_squared_error: 0.0668 - val_loss: 0.1069 - val_mean_squared_error: 0.0632\n",
      "Epoch 25/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1062 - mean_squared_error: 0.0628 - val_loss: 0.1025 - val_mean_squared_error: 0.0601\n",
      "Epoch 26/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1060 - mean_squared_error: 0.0640 - val_loss: 0.1156 - val_mean_squared_error: 0.0737\n",
      "Epoch 27/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1110 - mean_squared_error: 0.0697 - val_loss: 0.0977 - val_mean_squared_error: 0.0571\n",
      "Epoch 28/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1049 - mean_squared_error: 0.0649 - val_loss: 0.0952 - val_mean_squared_error: 0.0559\n",
      "Epoch 29/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1082 - mean_squared_error: 0.0675 - val_loss: 0.1182 - val_mean_squared_error: 0.0779\n",
      "Epoch 30/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1098 - mean_squared_error: 0.0703 - val_loss: 0.1003 - val_mean_squared_error: 0.0615\n",
      "Epoch 31/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1069 - mean_squared_error: 0.0681 - val_loss: 0.1060 - val_mean_squared_error: 0.0670\n",
      "Epoch 32/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1040 - mean_squared_error: 0.0650 - val_loss: 0.1055 - val_mean_squared_error: 0.0670\n",
      "Epoch 33/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1031 - mean_squared_error: 0.0639 - val_loss: 0.1122 - val_mean_squared_error: 0.0729\n",
      "Epoch 34/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0997 - mean_squared_error: 0.0609 - val_loss: 0.1004 - val_mean_squared_error: 0.0617\n",
      "Epoch 35/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1020 - mean_squared_error: 0.0633 - val_loss: 0.1133 - val_mean_squared_error: 0.0749\n",
      "Epoch 36/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1016 - mean_squared_error: 0.0634 - val_loss: 0.1075 - val_mean_squared_error: 0.0698\n",
      "Epoch 37/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1076 - mean_squared_error: 0.0697 - val_loss: 0.0934 - val_mean_squared_error: 0.0559\n",
      "Epoch 38/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1048 - mean_squared_error: 0.0671 - val_loss: 0.1121 - val_mean_squared_error: 0.0747\n",
      "Epoch 39/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1140 - mean_squared_error: 0.0758 - val_loss: 0.1097 - val_mean_squared_error: 0.0721\n",
      "Epoch 40/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0990 - mean_squared_error: 0.0614 - val_loss: 0.0992 - val_mean_squared_error: 0.0617\n",
      "Epoch 41/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0995 - mean_squared_error: 0.0620 - val_loss: 0.1023 - val_mean_squared_error: 0.0652\n",
      "Epoch 42/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0978 - mean_squared_error: 0.0612 - val_loss: 0.0956 - val_mean_squared_error: 0.0598\n",
      "Epoch 43/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0946 - mean_squared_error: 0.0593 - val_loss: 0.1026 - val_mean_squared_error: 0.0671\n",
      "Epoch 44/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0948 - mean_squared_error: 0.0595 - val_loss: 0.1020 - val_mean_squared_error: 0.0668\n",
      "Epoch 45/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0927 - mean_squared_error: 0.0582 - val_loss: 0.1013 - val_mean_squared_error: 0.0665\n",
      "Epoch 46/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0952 - mean_squared_error: 0.0605 - val_loss: 0.1010 - val_mean_squared_error: 0.0666\n",
      "Epoch 47/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0966 - mean_squared_error: 0.0623 - val_loss: 0.1001 - val_mean_squared_error: 0.0661\n",
      "Epoch 48/200\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.0954 - mean_squared_error: 0.0610 - val_loss: 0.1237 - val_mean_squared_error: 0.0890\n",
      "Epoch 49/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1022 - mean_squared_error: 0.0676 - val_loss: 0.0930 - val_mean_squared_error: 0.0582\n",
      "Epoch 50/200\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0959 - mean_squared_error: 0.0615 - val_loss: 0.0980 - val_mean_squared_error: 0.0633\n",
      "Epoch 51/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0930 - mean_squared_error: 0.0591 - val_loss: 0.0996 - val_mean_squared_error: 0.0663\n",
      "Epoch 52/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1029 - mean_squared_error: 0.0692 - val_loss: 0.0982 - val_mean_squared_error: 0.0652\n",
      "Epoch 53/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0934 - mean_squared_error: 0.0597 - val_loss: 0.0907 - val_mean_squared_error: 0.0578\n",
      "Epoch 54/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0913 - mean_squared_error: 0.0586 - val_loss: 0.0964 - val_mean_squared_error: 0.0636\n",
      "Epoch 55/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0908 - mean_squared_error: 0.0585 - val_loss: 0.1172 - val_mean_squared_error: 0.0851\n",
      "Epoch 56/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0908 - mean_squared_error: 0.0592 - val_loss: 0.1713 - val_mean_squared_error: 0.1388\n",
      "Epoch 57/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1011 - mean_squared_error: 0.0693 - val_loss: 0.0987 - val_mean_squared_error: 0.0668\n",
      "Epoch 58/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0921 - mean_squared_error: 0.0605 - val_loss: 0.1109 - val_mean_squared_error: 0.0791\n",
      "Epoch 59/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0936 - mean_squared_error: 0.0622 - val_loss: 0.1276 - val_mean_squared_error: 0.0955\n",
      "Epoch 60/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0911 - mean_squared_error: 0.0596 - val_loss: 0.1053 - val_mean_squared_error: 0.0737\n",
      "Epoch 61/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0912 - mean_squared_error: 0.0600 - val_loss: 0.1078 - val_mean_squared_error: 0.0758\n",
      "Epoch 62/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0901 - mean_squared_error: 0.0583 - val_loss: 0.1045 - val_mean_squared_error: 0.0731\n",
      "Epoch 63/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0960 - mean_squared_error: 0.0651 - val_loss: 0.1119 - val_mean_squared_error: 0.0808\n",
      "Epoch 64/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0907 - mean_squared_error: 0.0603 - val_loss: 0.0914 - val_mean_squared_error: 0.0612\n",
      "Epoch 65/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0893 - mean_squared_error: 0.0592 - val_loss: 0.0913 - val_mean_squared_error: 0.0611\n",
      "Epoch 66/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0991 - mean_squared_error: 0.0689 - val_loss: 0.1008 - val_mean_squared_error: 0.0701\n",
      "Epoch 67/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1010 - mean_squared_error: 0.0710 - val_loss: 0.0903 - val_mean_squared_error: 0.0608\n",
      "Epoch 68/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0914 - mean_squared_error: 0.0617 - val_loss: 0.0880 - val_mean_squared_error: 0.0583\n",
      "Epoch 69/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0882 - mean_squared_error: 0.0585 - val_loss: 0.0837 - val_mean_squared_error: 0.0545\n",
      "Epoch 70/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0913 - mean_squared_error: 0.0618 - val_loss: 0.0978 - val_mean_squared_error: 0.0680\n",
      "Epoch 71/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0895 - mean_squared_error: 0.0598 - val_loss: 0.0989 - val_mean_squared_error: 0.0696\n",
      "Epoch 72/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0838 - mean_squared_error: 0.0545 - val_loss: 0.0864 - val_mean_squared_error: 0.0569\n",
      "Epoch 73/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0881 - mean_squared_error: 0.0588 - val_loss: 0.1021 - val_mean_squared_error: 0.0736\n",
      "Epoch 74/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0877 - mean_squared_error: 0.0588 - val_loss: 0.1129 - val_mean_squared_error: 0.0836\n",
      "Epoch 75/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0873 - mean_squared_error: 0.0586 - val_loss: 0.1136 - val_mean_squared_error: 0.0848\n",
      "Epoch 76/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0928 - mean_squared_error: 0.0636 - val_loss: 0.0972 - val_mean_squared_error: 0.0676\n",
      "Epoch 77/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0817 - mean_squared_error: 0.0527 - val_loss: 0.1025 - val_mean_squared_error: 0.0738\n",
      "Epoch 78/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0877 - mean_squared_error: 0.0582 - val_loss: 0.0973 - val_mean_squared_error: 0.0682\n",
      "Epoch 79/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0853 - mean_squared_error: 0.0570 - val_loss: 0.1013 - val_mean_squared_error: 0.0735\n",
      "Epoch 80/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1306 - mean_squared_error: 0.0977 - val_loss: 0.1380 - val_mean_squared_error: 0.1039\n",
      "Epoch 81/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1001 - mean_squared_error: 0.0674 - val_loss: 0.1124 - val_mean_squared_error: 0.0808\n",
      "Epoch 82/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0898 - mean_squared_error: 0.0590 - val_loss: 0.1041 - val_mean_squared_error: 0.0737\n",
      "Epoch 83/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0832 - mean_squared_error: 0.0530 - val_loss: 0.1019 - val_mean_squared_error: 0.0715\n",
      "Epoch 84/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0865 - mean_squared_error: 0.0568 - val_loss: 0.0995 - val_mean_squared_error: 0.0701\n",
      "Epoch 85/200\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 0.0824 - mean_squared_error: 0.0528 - val_loss: 0.0939 - val_mean_squared_error: 0.0641\n",
      "Epoch 86/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0875 - mean_squared_error: 0.0581 - val_loss: 0.1039 - val_mean_squared_error: 0.0745\n",
      "Epoch 87/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0882 - mean_squared_error: 0.0587 - val_loss: 0.1245 - val_mean_squared_error: 0.0948\n",
      "Epoch 88/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0837 - mean_squared_error: 0.0534 - val_loss: 0.0922 - val_mean_squared_error: 0.0625\n",
      "Epoch 89/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0835 - mean_squared_error: 0.0542 - val_loss: 0.0879 - val_mean_squared_error: 0.0587\n",
      "Epoch 90/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0788 - mean_squared_error: 0.0499 - val_loss: 0.1135 - val_mean_squared_error: 0.0845\n",
      "Epoch 91/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0809 - mean_squared_error: 0.0517 - val_loss: 0.1066 - val_mean_squared_error: 0.0775\n",
      "Epoch 92/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0808 - mean_squared_error: 0.0517 - val_loss: 0.1039 - val_mean_squared_error: 0.0744\n",
      "Epoch 93/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0806 - mean_squared_error: 0.0511 - val_loss: 0.0891 - val_mean_squared_error: 0.0597\n",
      "Epoch 94/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0826 - mean_squared_error: 0.0535 - val_loss: 0.1010 - val_mean_squared_error: 0.0720\n",
      "Epoch 95/200\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0776 - mean_squared_error: 0.0479 - val_loss: 0.1006 - val_mean_squared_error: 0.0711\n",
      "Epoch 96/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0818 - mean_squared_error: 0.0522 - val_loss: 0.0884 - val_mean_squared_error: 0.0590\n",
      "Epoch 97/200\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.0836 - mean_squared_error: 0.0542 - val_loss: 0.1064 - val_mean_squared_error: 0.0770\n",
      "Epoch 98/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0816 - mean_squared_error: 0.0521 - val_loss: 0.0968 - val_mean_squared_error: 0.0673\n",
      "Epoch 99/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0799 - mean_squared_error: 0.0504 - val_loss: 0.1037 - val_mean_squared_error: 0.0744\n",
      "Epoch 100/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0830 - mean_squared_error: 0.0534 - val_loss: 0.1061 - val_mean_squared_error: 0.0769\n",
      "Epoch 101/200\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.0773 - mean_squared_error: 0.0475 - val_loss: 0.1021 - val_mean_squared_error: 0.0726\n",
      "Epoch 102/200\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.0831 - mean_squared_error: 0.0529 - val_loss: 0.1131 - val_mean_squared_error: 0.0836\n",
      "Epoch 103/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0861 - mean_squared_error: 0.0565 - val_loss: 0.1060 - val_mean_squared_error: 0.0761\n",
      "Epoch 104/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0829 - mean_squared_error: 0.0532 - val_loss: 0.1030 - val_mean_squared_error: 0.0732\n",
      "Epoch 105/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0810 - mean_squared_error: 0.0511 - val_loss: 0.0989 - val_mean_squared_error: 0.0690\n",
      "Epoch 106/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0853 - mean_squared_error: 0.0558 - val_loss: 0.1099 - val_mean_squared_error: 0.0807\n",
      "Epoch 107/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0871 - mean_squared_error: 0.0577 - val_loss: 0.1098 - val_mean_squared_error: 0.0801\n",
      "Epoch 108/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0766 - mean_squared_error: 0.0471 - val_loss: 0.1035 - val_mean_squared_error: 0.0745\n",
      "Epoch 109/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0786 - mean_squared_error: 0.0494 - val_loss: 0.1008 - val_mean_squared_error: 0.0718\n",
      "Epoch 110/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0791 - mean_squared_error: 0.0503 - val_loss: 0.1046 - val_mean_squared_error: 0.0752\n",
      "Epoch 111/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0840 - mean_squared_error: 0.0547 - val_loss: 0.1005 - val_mean_squared_error: 0.0712\n",
      "Epoch 112/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0811 - mean_squared_error: 0.0517 - val_loss: 0.1025 - val_mean_squared_error: 0.0731\n",
      "Epoch 113/200\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.0820 - mean_squared_error: 0.0522 - val_loss: 0.0978 - val_mean_squared_error: 0.0682\n",
      "Epoch 114/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0778 - mean_squared_error: 0.0480 - val_loss: 0.0977 - val_mean_squared_error: 0.0678\n",
      "Epoch 115/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0794 - mean_squared_error: 0.0499 - val_loss: 0.1202 - val_mean_squared_error: 0.0908\n",
      "Epoch 116/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0885 - mean_squared_error: 0.0589 - val_loss: 0.1258 - val_mean_squared_error: 0.0960\n",
      "Epoch 117/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0815 - mean_squared_error: 0.0519 - val_loss: 0.0963 - val_mean_squared_error: 0.0668\n",
      "Epoch 118/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0801 - mean_squared_error: 0.0509 - val_loss: 0.1045 - val_mean_squared_error: 0.0747\n",
      "Epoch 119/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0788 - mean_squared_error: 0.0488 - val_loss: 0.0956 - val_mean_squared_error: 0.0661\n",
      "Epoch 120/200\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.0746 - mean_squared_error: 0.0452 - val_loss: 0.1318 - val_mean_squared_error: 0.1025\n",
      "Epoch 121/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0816 - mean_squared_error: 0.0520 - val_loss: 0.1056 - val_mean_squared_error: 0.0764\n",
      "Epoch 122/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0808 - mean_squared_error: 0.0517 - val_loss: 0.0948 - val_mean_squared_error: 0.0656\n",
      "Epoch 123/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0850 - mean_squared_error: 0.0557 - val_loss: 0.0946 - val_mean_squared_error: 0.0659\n",
      "Epoch 124/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0747 - mean_squared_error: 0.0456 - val_loss: 0.0999 - val_mean_squared_error: 0.0710\n",
      "Epoch 125/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0824 - mean_squared_error: 0.0535 - val_loss: 0.0903 - val_mean_squared_error: 0.0611\n",
      "Epoch 126/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0816 - mean_squared_error: 0.0524 - val_loss: 0.0975 - val_mean_squared_error: 0.0681\n",
      "Epoch 127/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0810 - mean_squared_error: 0.0517 - val_loss: 0.0910 - val_mean_squared_error: 0.0617\n",
      "Epoch 128/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0799 - mean_squared_error: 0.0506 - val_loss: 0.1068 - val_mean_squared_error: 0.0772\n",
      "Epoch 129/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0801 - mean_squared_error: 0.0507 - val_loss: 0.0942 - val_mean_squared_error: 0.0649\n",
      "Epoch 130/200\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.0808 - mean_squared_error: 0.0515 - val_loss: 0.1078 - val_mean_squared_error: 0.0786\n",
      "Epoch 131/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0785 - mean_squared_error: 0.0493 - val_loss: 0.0913 - val_mean_squared_error: 0.0617\n",
      "Epoch 132/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0779 - mean_squared_error: 0.0490 - val_loss: 0.1041 - val_mean_squared_error: 0.0753\n",
      "Epoch 133/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.1295 - mean_squared_error: 0.0973 - val_loss: 0.1343 - val_mean_squared_error: 0.1017\n",
      "Epoch 134/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0899 - mean_squared_error: 0.0580 - val_loss: 0.1113 - val_mean_squared_error: 0.0804\n",
      "Epoch 135/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0797 - mean_squared_error: 0.0487 - val_loss: 0.1008 - val_mean_squared_error: 0.0701\n",
      "Epoch 136/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0787 - mean_squared_error: 0.0481 - val_loss: 0.1034 - val_mean_squared_error: 0.0729\n",
      "Epoch 137/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0776 - mean_squared_error: 0.0476 - val_loss: 0.1193 - val_mean_squared_error: 0.0891\n",
      "Epoch 138/200\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 0.0902 - mean_squared_error: 0.0595 - val_loss: 0.1057 - val_mean_squared_error: 0.0750\n",
      "Epoch 139/200\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.0863 - mean_squared_error: 0.0556 - val_loss: 0.0968 - val_mean_squared_error: 0.0662\n",
      "Epoch 140/200\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.0757 - mean_squared_error: 0.0452 - val_loss: 0.1005 - val_mean_squared_error: 0.0703\n",
      "Epoch 141/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0791 - mean_squared_error: 0.0492 - val_loss: 0.1107 - val_mean_squared_error: 0.0807\n",
      "Epoch 142/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0827 - mean_squared_error: 0.0523 - val_loss: 0.0897 - val_mean_squared_error: 0.0596\n",
      "Epoch 143/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0798 - mean_squared_error: 0.0498 - val_loss: 0.1035 - val_mean_squared_error: 0.0735\n",
      "Epoch 144/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0779 - mean_squared_error: 0.0478 - val_loss: 0.0971 - val_mean_squared_error: 0.0673\n",
      "Epoch 145/200\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.0724 - mean_squared_error: 0.0428 - val_loss: 0.1006 - val_mean_squared_error: 0.0712\n",
      "Epoch 146/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0786 - mean_squared_error: 0.0487 - val_loss: 0.1016 - val_mean_squared_error: 0.0721\n",
      "Epoch 147/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0755 - mean_squared_error: 0.0459 - val_loss: 0.1050 - val_mean_squared_error: 0.0756\n",
      "Epoch 148/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0767 - mean_squared_error: 0.0470 - val_loss: 0.0946 - val_mean_squared_error: 0.0648\n",
      "Epoch 149/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0777 - mean_squared_error: 0.0482 - val_loss: 0.0946 - val_mean_squared_error: 0.0653\n",
      "Epoch 150/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0780 - mean_squared_error: 0.0486 - val_loss: 0.0966 - val_mean_squared_error: 0.0673\n",
      "Epoch 151/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0803 - mean_squared_error: 0.0510 - val_loss: 0.1088 - val_mean_squared_error: 0.0793\n",
      "Epoch 152/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0796 - mean_squared_error: 0.0505 - val_loss: 0.0989 - val_mean_squared_error: 0.0696\n",
      "Epoch 153/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0761 - mean_squared_error: 0.0471 - val_loss: 0.1102 - val_mean_squared_error: 0.0815\n",
      "Epoch 154/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0731 - mean_squared_error: 0.0442 - val_loss: 0.0879 - val_mean_squared_error: 0.0590\n",
      "Epoch 155/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0738 - mean_squared_error: 0.0452 - val_loss: 0.0912 - val_mean_squared_error: 0.0623\n",
      "Epoch 156/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0805 - mean_squared_error: 0.0517 - val_loss: 0.1169 - val_mean_squared_error: 0.0878\n",
      "Epoch 157/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0821 - mean_squared_error: 0.0530 - val_loss: 0.1009 - val_mean_squared_error: 0.0719\n",
      "Epoch 158/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0767 - mean_squared_error: 0.0479 - val_loss: 0.0988 - val_mean_squared_error: 0.0704\n",
      "Epoch 159/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0820 - mean_squared_error: 0.0534 - val_loss: 0.1027 - val_mean_squared_error: 0.0737\n",
      "Epoch 160/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0816 - mean_squared_error: 0.0526 - val_loss: 0.0924 - val_mean_squared_error: 0.0633\n",
      "Epoch 161/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0774 - mean_squared_error: 0.0485 - val_loss: 0.1271 - val_mean_squared_error: 0.0985\n",
      "Epoch 162/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0803 - mean_squared_error: 0.0518 - val_loss: 0.1020 - val_mean_squared_error: 0.0733\n",
      "Epoch 163/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0812 - mean_squared_error: 0.0525 - val_loss: 0.1289 - val_mean_squared_error: 0.1000\n",
      "Epoch 164/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0786 - mean_squared_error: 0.0496 - val_loss: 0.1127 - val_mean_squared_error: 0.0838\n",
      "Epoch 165/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0767 - mean_squared_error: 0.0476 - val_loss: 0.1093 - val_mean_squared_error: 0.0804\n",
      "Epoch 166/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0839 - mean_squared_error: 0.0552 - val_loss: 0.0893 - val_mean_squared_error: 0.0607\n",
      "Epoch 167/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0759 - mean_squared_error: 0.0474 - val_loss: 0.0920 - val_mean_squared_error: 0.0633\n",
      "Epoch 168/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0789 - mean_squared_error: 0.0501 - val_loss: 0.0955 - val_mean_squared_error: 0.0671\n",
      "Epoch 169/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0797 - mean_squared_error: 0.0510 - val_loss: 0.1001 - val_mean_squared_error: 0.0717\n",
      "Epoch 170/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0787 - mean_squared_error: 0.0506 - val_loss: 0.1074 - val_mean_squared_error: 0.0793\n",
      "Epoch 171/200\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.0832 - mean_squared_error: 0.0547 - val_loss: 0.1009 - val_mean_squared_error: 0.0723\n",
      "Epoch 172/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 0.0792 - mean_squared_error: 0.0512"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-4c9fa70d6889>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Train the model on the normalized training set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m epochs, mse = train_model(my_model, train_df, epochs, \n\u001b[0;32m---> 13\u001b[0;31m                           label_name, batch_size, validation_split=0.1)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mplot_the_loss_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-4af0d426489c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataset, epochs, label_name, batch_size, validation_split)\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   history = model.fit(x=features, y=label, batch_size=batch_size,\n\u001b[0;32m----> 9\u001b[0;31m                       epochs=epochs, shuffle=True, validation_split=validation_split) \n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m# The list of epochs is stored separately from the rest of history.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1429\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1430\u001b[0m               \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1431\u001b[0;31m               _use_cached_eval_dataset=True)\n\u001b[0m\u001b[1;32m   1432\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1708\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1710\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1711\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1712\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m             \"not be specified.\")\n\u001b[0;32m--> 755\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    785\u001b[0m                 \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                 output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m         \u001b[0;31m# Delete the resource when this object is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3314\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3315\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 3316\u001b[0;31m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0m\u001b[1;32m   3317\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3318\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "batch_size = 10\n",
    "\n",
    "label_name = \"PERF\"\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model(learning_rate, feature_layer)\n",
    "\n",
    "# Train the model on the normalized training set.\n",
    "epochs, mse = train_model(my_model, train_df, epochs, \n",
    "                          label_name, batch_size, validation_split=0.1)\n",
    "plot_the_loss_curve(epochs, mse)\n",
    "\n",
    "test_features = {name:np.array(value) for name, value in test_df.items()}\n",
    "test_label = np.array(test_features.pop(label_name)) # isolate the label\n",
    "print(\"\\n Evaluate the new model against the test set:\")\n",
    "my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
