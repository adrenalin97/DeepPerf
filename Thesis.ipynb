{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run on TensorFlow 2.x\n",
    "%tensorflow_version 2.x\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import relevant modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "from mlp_sparse_model import MLPSparseModel\n",
    "from mlp_plain_model import MLPPlainModel\n",
    "import time\n",
    "\n",
    "# The following lines adjust the granularity of reporting. \n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.7f}\".format\n",
    "\n",
    "# The following line improves formatting when ouputting NumPy arrays.\n",
    "np.set_printoptions(linewidth = 200)\n",
    "\n",
    "SAMPLE_SIZE = 11\n",
    "N_EXP = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_generator():\n",
    "    # Generate the initial seed for each sample size (to match the seed\n",
    "    # of the results in the paper)\n",
    "    # This is just the initial seed, for each experiment, the seeds will be\n",
    "    # equal the initial seed + the number of the experiment\n",
    "\n",
    "    N_train_all = np.multiply(11, [1, 2, 3, 4, 5])  # This is for LLVM\n",
    "    if SAMPLE_SIZE in N_train_all:\n",
    "        seed_o = np.where(N_train_all == SAMPLE_SIZE)[0][0]\n",
    "    else:\n",
    "        seed_o = np.random.randint(1, 101)\n",
    "\n",
    "    return seed_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Get data\n",
    "data_df = pd.read_csv(\"Data/LLVM_AllNumeric.csv\")\n",
    "column_dict = {name: bool for name in list(data_df.columns.values) if name != 'PERF'}\n",
    "column_dict['PERF'] = \"float64\"\n",
    "data_df = data_df.astype(column_dict)\n",
    "data_df = data_df.reindex(np.random.permutation(data_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gvn</th>\n",
       "      <th>instcombine</th>\n",
       "      <th>inline</th>\n",
       "      <th>jump_threading</th>\n",
       "      <th>simplifycfg</th>\n",
       "      <th>sccp</th>\n",
       "      <th>print_used_types</th>\n",
       "      <th>ipsccp</th>\n",
       "      <th>iv_users</th>\n",
       "      <th>licm</th>\n",
       "      <th>PERF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>0.5190492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>0.7934483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>0.8375933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>-1.1817979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>-0.2475572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>-0.1180392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>0.3634720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>1.0577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>1.0810716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>0.9995116</td>\n",
       "      <td>-0.9995116</td>\n",
       "      <td>-1.8395002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1024 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           gvn  instcombine     inline  ...   iv_users       licm       PERF\n",
       "672  0.9995116   -0.9995116  0.9995116  ...  0.9995116  0.9995116  0.5190492\n",
       "445  0.9995116    0.9995116  0.9995116  ... -0.9995116 -0.9995116  0.7934483\n",
       "731  0.9995116   -0.9995116  0.9995116  ...  0.9995116  0.9995116  0.8375933\n",
       "489 -0.9995116   -0.9995116 -0.9995116  ... -0.9995116  0.9995116 -1.1817979\n",
       "781 -0.9995116    0.9995116 -0.9995116  ...  0.9995116  0.9995116 -0.2475572\n",
       "..         ...          ...        ...  ...        ...        ...        ...\n",
       "273  0.9995116    0.9995116 -0.9995116  ...  0.9995116 -0.9995116 -0.1180392\n",
       "315  0.9995116    0.9995116 -0.9995116  ...  0.9995116 -0.9995116  0.3634720\n",
       "400  0.9995116    0.9995116  0.9995116  ... -0.9995116 -0.9995116  1.0577350\n",
       "759  0.9995116   -0.9995116  0.9995116  ...  0.9995116  0.9995116  1.0810716\n",
       "170 -0.9995116   -0.9995116 -0.9995116  ...  0.9995116 -0.9995116 -1.8395002\n",
       "\n",
       "[1024 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Normalize values\n",
    "data_df_mean = data_df.mean()\n",
    "data_df_std = data_df.std()\n",
    "data_df_norm = (data_df - data_df_mean)/data_df_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data set and set seed\n",
    "seed_init = seed_generator()\n",
    "seed = seed_init*N_EXP + 1\n",
    "np.random.seed(seed_init)\n",
    "train_df = data_df.sample(frac=0.6)\n",
    "test_df =data_df.drop(train_df.index).sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature layer\n",
    "columns = [column for column in column_dict.keys() if column != 'PERF']\n",
    "feature_columns = []\n",
    "for column in columns:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(column))\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define the plotting function.\n",
    "\n",
    "def plot_the_loss_curve(epochs, mse):\n",
    "  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Mean Squared Error\")\n",
    "\n",
    "  plt.plot(epochs, mse, label=\"Loss\")\n",
    "  plt.legend()\n",
    "  plt.ylim([mse.min()*0.95, mse.max() * 1.03])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Double-click for a possible solution\n",
    "\n",
    "# The following \"solution\" uses L2 regularization to bring training loss\n",
    "# and test loss closer to each other. Many, many other solutions are possible.\n",
    "\n",
    "\n",
    "def create_model(learning_rate, feature_layer):\n",
    "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
    "\n",
    "  # Discard any pre-existing version of the model.\n",
    "  model = None\n",
    "\n",
    "  # Most simple tf.keras models are sequential.\n",
    "  model = tf.keras.models.Sequential()\n",
    "\n",
    "  # Add the layer containing the feature columns to the model.\n",
    "  model.add(feature_layer)\n",
    "\n",
    "  # Describe the topography of the model. \n",
    "\n",
    "  # Implement L2 regularization in the first hidden layer.\n",
    "  model.add(tf.keras.layers.Dense(units=20, \n",
    "                                  activation='relu',\n",
    "                                  # kernel_regularizer=tf.keras.regularizers.l1(0.0001),\n",
    "                                  name='Hidden1'))\n",
    "  \n",
    "  # Implement L2 regularization in the second hidden layer.\n",
    "  model.add(tf.keras.layers.Dense(units=12, \n",
    "                                  activation='relu', \n",
    "                                  # kernel_regularizer=tf.keras.regularizers.l2(0.0001),\n",
    "                                  name='Hidden2'))\n",
    "\n",
    "  # Define the output layer.\n",
    "  model.add(tf.keras.layers.Dense(units=1,  \n",
    "                                  name='Output'))                              \n",
    "  \n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "  return model     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, epochs, label_name,\n",
    "                batch_size=None, validation_split=0.1):\n",
    "  \"\"\"Train the model by feeding it data.\"\"\"\n",
    "\n",
    "  # Split the dataset into features and label.\n",
    "  features = {name:np.array(value) for name, value in dataset.items()}\n",
    "  label = np.array(features.pop(label_name))\n",
    "  history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=True, validation_split=validation_split) \n",
    "\n",
    "  # The list of epochs is stored separately from the rest of history.\n",
    "  epochs = history.epoch\n",
    "  \n",
    "  # To track the progression of training, gather a snapshot\n",
    "  # of the model's mean squared error at each epoch. \n",
    "  hist = pd.DataFrame(history.history)\n",
    "  mse = hist[\"mean_squared_error\"]\n",
    "\n",
    "  return epochs, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/140\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'gvn': <tf.Tensor 'IteratorGetNext:0' shape=(None,) dtype=bool>, 'instcombine': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=bool>, 'inline': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=bool>, 'jump_threading': <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=bool>, 'simplifycfg': <tf.Tensor 'IteratorGetNext:9' shape=(None,) dtype=bool>, 'sccp': <tf.Tensor 'IteratorGetNext:8' shape=(None,) dtype=bool>, 'print_used_types': <tf.Tensor 'IteratorGetNext:7' shape=(None,) dtype=bool>, 'ipsccp': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=bool>, 'iv_users': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=bool>, 'licm': <tf.Tensor 'IteratorGetNext:6' shape=(None,) dtype=bool>}. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'gvn': <tf.Tensor 'IteratorGetNext:0' shape=(None,) dtype=bool>, 'instcombine': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=bool>, 'inline': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=bool>, 'jump_threading': <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=bool>, 'simplifycfg': <tf.Tensor 'IteratorGetNext:9' shape=(None,) dtype=bool>, 'sccp': <tf.Tensor 'IteratorGetNext:8' shape=(None,) dtype=bool>, 'print_used_types': <tf.Tensor 'IteratorGetNext:7' shape=(None,) dtype=bool>, 'ipsccp': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=bool>, 'iv_users': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=bool>, 'licm': <tf.Tensor 'IteratorGetNext:6' shape=(None,) dtype=bool>}. Consider rewriting this model with the Functional API.\n",
      "53/56 [===========================>..] - ETA: 0s - loss: 55723.3047 - mean_squared_error: 55722.1211WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'gvn': <tf.Tensor 'IteratorGetNext:0' shape=(None,) dtype=bool>, 'instcombine': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=bool>, 'inline': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=bool>, 'jump_threading': <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=bool>, 'simplifycfg': <tf.Tensor 'IteratorGetNext:9' shape=(None,) dtype=bool>, 'sccp': <tf.Tensor 'IteratorGetNext:8' shape=(None,) dtype=bool>, 'print_used_types': <tf.Tensor 'IteratorGetNext:7' shape=(None,) dtype=bool>, 'ipsccp': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=bool>, 'iv_users': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=bool>, 'licm': <tf.Tensor 'IteratorGetNext:6' shape=(None,) dtype=bool>}. Consider rewriting this model with the Functional API.\n",
      "56/56 [==============================] - 1s 8ms/step - loss: 55748.5703 - mean_squared_error: 55747.3828 - val_loss: 55909.0781 - val_mean_squared_error: 55907.8750\n",
      "Epoch 2/140\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 53884.4258 - mean_squared_error: 53883.1719 - val_loss: 52728.5234 - val_mean_squared_error: 52727.1797\n",
      "Epoch 3/140\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 49040.2188 - mean_squared_error: 49038.7148 - val_loss: 45091.0078 - val_mean_squared_error: 45089.3008\n",
      "Epoch 4/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 38895.5078 - mean_squared_error: 38893.5117 - val_loss: 31320.8027 - val_mean_squared_error: 31318.4590\n",
      "Epoch 5/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 23903.2168 - mean_squared_error: 23900.4805 - val_loss: 14694.5371 - val_mean_squared_error: 14691.3418\n",
      "Epoch 6/140\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 9811.0908 - mean_squared_error: 9807.4570 - val_loss: 4088.5896 - val_mean_squared_error: 4084.5229\n",
      "Epoch 7/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 3481.0012 - mean_squared_error: 3476.6494 - val_loss: 2025.5944 - val_mean_squared_error: 2021.0369\n",
      "Epoch 8/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 2578.6670 - mean_squared_error: 2574.0374 - val_loss: 2007.6953 - val_mean_squared_error: 2003.0352\n",
      "Epoch 9/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 2502.8525 - mean_squared_error: 2498.1941 - val_loss: 1971.2310 - val_mean_squared_error: 1966.5868\n",
      "Epoch 10/140\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 2447.7686 - mean_squared_error: 2443.1418 - val_loss: 1938.9266 - val_mean_squared_error: 1934.2920\n",
      "Epoch 11/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 2395.4597 - mean_squared_error: 2390.8352 - val_loss: 1909.2365 - val_mean_squared_error: 1904.6099\n",
      "Epoch 12/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 2343.9209 - mean_squared_error: 2339.2930 - val_loss: 1842.8787 - val_mean_squared_error: 1838.2903\n",
      "Epoch 13/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 2288.2615 - mean_squared_error: 2283.6997 - val_loss: 1799.5026 - val_mean_squared_error: 1794.9305\n",
      "Epoch 14/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 2225.7739 - mean_squared_error: 2221.1904 - val_loss: 1781.3158 - val_mean_squared_error: 1776.7384\n",
      "Epoch 15/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 2169.9280 - mean_squared_error: 2165.3623 - val_loss: 1742.0317 - val_mean_squared_error: 1737.4662\n",
      "Epoch 16/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 2110.7415 - mean_squared_error: 2106.1812 - val_loss: 1689.8390 - val_mean_squared_error: 1685.2933\n",
      "Epoch 17/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 2058.2273 - mean_squared_error: 2053.7090 - val_loss: 1633.0602 - val_mean_squared_error: 1628.5385\n",
      "Epoch 18/140\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 1996.8650 - mean_squared_error: 1992.3311 - val_loss: 1600.9264 - val_mean_squared_error: 1596.4030\n",
      "Epoch 19/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 1933.4600 - mean_squared_error: 1928.9357 - val_loss: 1554.1748 - val_mean_squared_error: 1549.6620\n",
      "Epoch 20/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 1871.2904 - mean_squared_error: 1866.7896 - val_loss: 1513.4998 - val_mean_squared_error: 1508.9907\n",
      "Epoch 21/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 1812.9923 - mean_squared_error: 1808.4897 - val_loss: 1475.4514 - val_mean_squared_error: 1470.9414\n",
      "Epoch 22/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 1745.5861 - mean_squared_error: 1741.0710 - val_loss: 1405.4464 - val_mean_squared_error: 1400.9573\n",
      "Epoch 23/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 1683.9005 - mean_squared_error: 1679.4205 - val_loss: 1377.3881 - val_mean_squared_error: 1372.8807\n",
      "Epoch 24/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 1617.0665 - mean_squared_error: 1612.5558 - val_loss: 1297.0439 - val_mean_squared_error: 1292.5626\n",
      "Epoch 25/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 1554.8196 - mean_squared_error: 1550.3267 - val_loss: 1245.6812 - val_mean_squared_error: 1241.1967\n",
      "Epoch 26/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 1488.4829 - mean_squared_error: 1483.9912 - val_loss: 1210.9502 - val_mean_squared_error: 1206.4442\n",
      "Epoch 27/140\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 1419.6481 - mean_squared_error: 1415.1309 - val_loss: 1144.3306 - val_mean_squared_error: 1139.8289\n",
      "Epoch 28/140\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 1350.0128 - mean_squared_error: 1345.4965 - val_loss: 1103.3098 - val_mean_squared_error: 1098.7814\n",
      "Epoch 29/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 1284.5992 - mean_squared_error: 1280.0658 - val_loss: 1031.5543 - val_mean_squared_error: 1027.0314\n",
      "Epoch 30/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 1214.8757 - mean_squared_error: 1210.3370 - val_loss: 1021.4821 - val_mean_squared_error: 1016.9052\n",
      "Epoch 31/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 1150.3397 - mean_squared_error: 1145.7670 - val_loss: 930.9960 - val_mean_squared_error: 926.4247\n",
      "Epoch 32/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 1083.8969 - mean_squared_error: 1079.3099 - val_loss: 879.1287 - val_mean_squared_error: 874.5326\n",
      "Epoch 33/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 1016.2933 - mean_squared_error: 1011.6685 - val_loss: 816.1552 - val_mean_squared_error: 811.5391\n",
      "Epoch 34/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 948.6349 - mean_squared_error: 943.9914 - val_loss: 770.6254 - val_mean_squared_error: 765.9730\n",
      "Epoch 35/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 885.2467 - mean_squared_error: 880.5780 - val_loss: 722.7823 - val_mean_squared_error: 718.0917\n",
      "Epoch 36/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 821.0294 - mean_squared_error: 816.3351 - val_loss: 688.2516 - val_mean_squared_error: 683.5085\n",
      "Epoch 37/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 755.8903 - mean_squared_error: 751.1361 - val_loss: 615.9997 - val_mean_squared_error: 611.2342\n",
      "Epoch 38/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 698.2703 - mean_squared_error: 693.4845 - val_loss: 588.4670 - val_mean_squared_error: 583.6368\n",
      "Epoch 39/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 634.0388 - mean_squared_error: 629.1996 - val_loss: 512.1931 - val_mean_squared_error: 507.3347\n",
      "Epoch 40/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 577.8887 - mean_squared_error: 572.9983 - val_loss: 462.3983 - val_mean_squared_error: 457.4915\n",
      "Epoch 41/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 523.1992 - mean_squared_error: 518.2712 - val_loss: 422.1711 - val_mean_squared_error: 417.2094\n",
      "Epoch 42/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 470.2484 - mean_squared_error: 465.2627 - val_loss: 379.1577 - val_mean_squared_error: 374.1378\n",
      "Epoch 43/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 420.4645 - mean_squared_error: 415.4204 - val_loss: 330.9048 - val_mean_squared_error: 325.8340\n",
      "Epoch 44/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 375.8215 - mean_squared_error: 370.7145 - val_loss: 291.0170 - val_mean_squared_error: 285.8922\n",
      "Epoch 45/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 331.2003 - mean_squared_error: 326.0390 - val_loss: 264.4977 - val_mean_squared_error: 259.3015\n",
      "Epoch 46/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 293.2697 - mean_squared_error: 288.0432 - val_loss: 232.5919 - val_mean_squared_error: 227.3321\n",
      "Epoch 47/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 257.2392 - mean_squared_error: 251.9571 - val_loss: 215.3431 - val_mean_squared_error: 210.0146\n",
      "Epoch 48/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 226.5900 - mean_squared_error: 221.2449 - val_loss: 172.1677 - val_mean_squared_error: 166.8013\n",
      "Epoch 49/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 200.5489 - mean_squared_error: 195.1516 - val_loss: 153.7172 - val_mean_squared_error: 148.2877\n",
      "Epoch 50/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 174.9537 - mean_squared_error: 169.4855 - val_loss: 127.0736 - val_mean_squared_error: 121.5883\n",
      "Epoch 51/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 154.6178 - mean_squared_error: 149.1048 - val_loss: 125.6134 - val_mean_squared_error: 120.0585\n",
      "Epoch 52/140\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 140.1136 - mean_squared_error: 134.5454 - val_loss: 104.2974 - val_mean_squared_error: 98.7006\n",
      "Epoch 53/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 124.3151 - mean_squared_error: 118.7085 - val_loss: 89.2301 - val_mean_squared_error: 83.5924\n",
      "Epoch 54/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 114.1808 - mean_squared_error: 108.5209 - val_loss: 80.8624 - val_mean_squared_error: 75.1773\n",
      "Epoch 55/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 103.6976 - mean_squared_error: 97.9990 - val_loss: 76.4658 - val_mean_squared_error: 70.7380\n",
      "Epoch 56/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 96.7084 - mean_squared_error: 90.9708 - val_loss: 64.8785 - val_mean_squared_error: 59.1218\n",
      "Epoch 57/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 90.5700 - mean_squared_error: 84.8030 - val_loss: 60.5446 - val_mean_squared_error: 54.7556\n",
      "Epoch 58/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 85.6077 - mean_squared_error: 79.8030 - val_loss: 52.1438 - val_mean_squared_error: 46.3407\n",
      "Epoch 59/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 84.1599 - mean_squared_error: 78.3341 - val_loss: 48.8101 - val_mean_squared_error: 42.9749\n",
      "Epoch 60/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 80.9549 - mean_squared_error: 75.1021 - val_loss: 53.3417 - val_mean_squared_error: 47.4692\n",
      "Epoch 61/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 78.6227 - mean_squared_error: 72.7522 - val_loss: 48.4479 - val_mean_squared_error: 42.5614\n",
      "Epoch 62/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 76.5293 - mean_squared_error: 70.6414 - val_loss: 42.9970 - val_mean_squared_error: 37.1074\n",
      "Epoch 63/140\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 76.2662 - mean_squared_error: 70.3633 - val_loss: 41.8918 - val_mean_squared_error: 35.9855\n",
      "Epoch 64/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 75.4769 - mean_squared_error: 69.5638 - val_loss: 43.0412 - val_mean_squared_error: 37.1162\n",
      "Epoch 65/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 75.9036 - mean_squared_error: 69.9747 - val_loss: 45.3749 - val_mean_squared_error: 39.4358\n",
      "Epoch 66/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 74.5617 - mean_squared_error: 68.6252 - val_loss: 41.0930 - val_mean_squared_error: 35.1513\n",
      "Epoch 67/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 73.6167 - mean_squared_error: 67.6762 - val_loss: 40.0991 - val_mean_squared_error: 34.1665\n",
      "Epoch 68/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 73.5343 - mean_squared_error: 67.5868 - val_loss: 39.7458 - val_mean_squared_error: 33.7954\n",
      "Epoch 69/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 73.8349 - mean_squared_error: 67.8840 - val_loss: 39.2381 - val_mean_squared_error: 33.2894\n",
      "Epoch 70/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 73.5508 - mean_squared_error: 67.5980 - val_loss: 39.6632 - val_mean_squared_error: 33.7039\n",
      "Epoch 71/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 72.8141 - mean_squared_error: 66.8494 - val_loss: 38.2457 - val_mean_squared_error: 32.2914\n",
      "Epoch 72/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 72.8842 - mean_squared_error: 66.9256 - val_loss: 37.7736 - val_mean_squared_error: 31.8125\n",
      "Epoch 73/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 72.3688 - mean_squared_error: 66.4072 - val_loss: 39.2751 - val_mean_squared_error: 33.3124\n",
      "Epoch 74/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 72.5337 - mean_squared_error: 66.5724 - val_loss: 43.4424 - val_mean_squared_error: 37.4705\n",
      "Epoch 75/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 72.9318 - mean_squared_error: 66.9650 - val_loss: 39.5178 - val_mean_squared_error: 33.5514\n",
      "Epoch 76/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 71.6023 - mean_squared_error: 65.6382 - val_loss: 37.5567 - val_mean_squared_error: 31.6067\n",
      "Epoch 77/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 72.3015 - mean_squared_error: 66.3434 - val_loss: 37.6771 - val_mean_squared_error: 31.7188\n",
      "Epoch 78/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 71.5323 - mean_squared_error: 65.5684 - val_loss: 40.0738 - val_mean_squared_error: 34.1050\n",
      "Epoch 79/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 71.5632 - mean_squared_error: 65.5950 - val_loss: 38.1762 - val_mean_squared_error: 32.2135\n",
      "Epoch 80/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 71.4288 - mean_squared_error: 65.4689 - val_loss: 38.3458 - val_mean_squared_error: 32.3834\n",
      "Epoch 81/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 71.6052 - mean_squared_error: 65.6418 - val_loss: 37.6645 - val_mean_squared_error: 31.7081\n",
      "Epoch 82/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 71.1038 - mean_squared_error: 65.1446 - val_loss: 37.2572 - val_mean_squared_error: 31.3010\n",
      "Epoch 83/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 71.1864 - mean_squared_error: 65.2277 - val_loss: 36.7760 - val_mean_squared_error: 30.8239\n",
      "Epoch 84/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 71.6693 - mean_squared_error: 65.7135 - val_loss: 38.7860 - val_mean_squared_error: 32.8258\n",
      "Epoch 85/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 70.8536 - mean_squared_error: 64.8970 - val_loss: 37.6605 - val_mean_squared_error: 31.7012\n",
      "Epoch 86/140\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 70.5414 - mean_squared_error: 64.5813 - val_loss: 38.9553 - val_mean_squared_error: 32.9948\n",
      "Epoch 87/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 70.3575 - mean_squared_error: 64.4022 - val_loss: 36.3125 - val_mean_squared_error: 30.3641\n",
      "Epoch 88/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 71.1454 - mean_squared_error: 65.1903 - val_loss: 37.1867 - val_mean_squared_error: 31.2277\n",
      "Epoch 89/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 70.0150 - mean_squared_error: 64.0578 - val_loss: 37.0724 - val_mean_squared_error: 31.1147\n",
      "Epoch 90/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 70.0261 - mean_squared_error: 64.0691 - val_loss: 37.9552 - val_mean_squared_error: 31.9985\n",
      "Epoch 91/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 69.7012 - mean_squared_error: 63.7430 - val_loss: 35.9779 - val_mean_squared_error: 30.0241\n",
      "Epoch 92/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 70.3891 - mean_squared_error: 64.4385 - val_loss: 37.3246 - val_mean_squared_error: 31.3722\n",
      "Epoch 93/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 69.0828 - mean_squared_error: 63.1274 - val_loss: 37.1485 - val_mean_squared_error: 31.1903\n",
      "Epoch 94/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 69.1221 - mean_squared_error: 63.1669 - val_loss: 38.3682 - val_mean_squared_error: 32.4307\n",
      "Epoch 95/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 69.2022 - mean_squared_error: 63.2530 - val_loss: 36.7821 - val_mean_squared_error: 30.8331\n",
      "Epoch 96/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 68.2292 - mean_squared_error: 62.2755 - val_loss: 38.8880 - val_mean_squared_error: 32.9258\n",
      "Epoch 97/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 67.6430 - mean_squared_error: 61.6921 - val_loss: 39.8545 - val_mean_squared_error: 33.8943\n",
      "Epoch 98/140\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 67.8493 - mean_squared_error: 61.8987 - val_loss: 38.7995 - val_mean_squared_error: 32.8424\n",
      "Epoch 99/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 66.7801 - mean_squared_error: 60.8298 - val_loss: 36.4522 - val_mean_squared_error: 30.4994\n",
      "Epoch 100/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 66.2673 - mean_squared_error: 60.3112 - val_loss: 35.4823 - val_mean_squared_error: 29.5250\n",
      "Epoch 101/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 65.9331 - mean_squared_error: 59.9785 - val_loss: 35.0315 - val_mean_squared_error: 29.0762\n",
      "Epoch 102/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 65.7283 - mean_squared_error: 59.7652 - val_loss: 35.1757 - val_mean_squared_error: 29.2227\n",
      "Epoch 103/140\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 64.4508 - mean_squared_error: 58.4847 - val_loss: 34.5931 - val_mean_squared_error: 28.6329\n",
      "Epoch 104/140\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 64.5050 - mean_squared_error: 58.5379 - val_loss: 36.6259 - val_mean_squared_error: 30.6761\n",
      "Epoch 105/140\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 64.7170 - mean_squared_error: 58.7503 - val_loss: 34.1348 - val_mean_squared_error: 28.1647\n",
      "Epoch 106/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 62.7648 - mean_squared_error: 56.7827 - val_loss: 36.4189 - val_mean_squared_error: 30.4269\n",
      "Epoch 107/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 62.8412 - mean_squared_error: 56.8547 - val_loss: 34.1560 - val_mean_squared_error: 28.1779\n",
      "Epoch 108/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 61.6538 - mean_squared_error: 55.6691 - val_loss: 34.3885 - val_mean_squared_error: 28.4082\n",
      "Epoch 109/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 61.4392 - mean_squared_error: 55.4474 - val_loss: 33.4491 - val_mean_squared_error: 27.4640\n",
      "Epoch 110/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 61.6226 - mean_squared_error: 55.6264 - val_loss: 37.6279 - val_mean_squared_error: 31.6168\n",
      "Epoch 111/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 60.1615 - mean_squared_error: 54.1573 - val_loss: 32.5334 - val_mean_squared_error: 26.5381\n",
      "Epoch 112/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 61.1017 - mean_squared_error: 55.0969 - val_loss: 32.0278 - val_mean_squared_error: 26.0191\n",
      "Epoch 113/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 59.4777 - mean_squared_error: 53.4549 - val_loss: 32.6623 - val_mean_squared_error: 26.6338\n",
      "Epoch 114/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 58.1336 - mean_squared_error: 52.1041 - val_loss: 31.5508 - val_mean_squared_error: 25.5328\n",
      "Epoch 115/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 59.1225 - mean_squared_error: 53.0934 - val_loss: 30.9023 - val_mean_squared_error: 24.8661\n",
      "Epoch 116/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 58.0558 - mean_squared_error: 52.0160 - val_loss: 31.2319 - val_mean_squared_error: 25.1881\n",
      "Epoch 117/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 57.0067 - mean_squared_error: 50.9462 - val_loss: 31.1739 - val_mean_squared_error: 25.1181\n",
      "Epoch 118/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 56.8651 - mean_squared_error: 50.7938 - val_loss: 31.2624 - val_mean_squared_error: 25.1838\n",
      "Epoch 119/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 56.0133 - mean_squared_error: 49.9454 - val_loss: 33.3846 - val_mean_squared_error: 27.2992\n",
      "Epoch 120/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 55.0999 - mean_squared_error: 49.0195 - val_loss: 28.9558 - val_mean_squared_error: 22.8753\n",
      "Epoch 121/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 54.5712 - mean_squared_error: 48.4815 - val_loss: 29.2278 - val_mean_squared_error: 23.1312\n",
      "Epoch 122/140\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 53.8861 - mean_squared_error: 47.8022 - val_loss: 36.6279 - val_mean_squared_error: 30.5195\n",
      "Epoch 123/140\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 54.2732 - mean_squared_error: 48.1692 - val_loss: 30.0926 - val_mean_squared_error: 23.9770\n",
      "Epoch 124/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 54.4230 - mean_squared_error: 48.3124 - val_loss: 30.0631 - val_mean_squared_error: 23.9419\n",
      "Epoch 125/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 53.9813 - mean_squared_error: 47.8530 - val_loss: 30.3237 - val_mean_squared_error: 24.1849\n",
      "Epoch 126/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 53.5091 - mean_squared_error: 47.3786 - val_loss: 28.3925 - val_mean_squared_error: 22.2567\n",
      "Epoch 127/140\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 50.7241 - mean_squared_error: 44.5821 - val_loss: 27.6806 - val_mean_squared_error: 21.5447\n",
      "Epoch 128/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 50.4792 - mean_squared_error: 44.3234 - val_loss: 25.9779 - val_mean_squared_error: 19.8198\n",
      "Epoch 129/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 51.0719 - mean_squared_error: 44.9113 - val_loss: 27.3414 - val_mean_squared_error: 21.1878\n",
      "Epoch 130/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 50.2676 - mean_squared_error: 44.0899 - val_loss: 26.2902 - val_mean_squared_error: 20.1093\n",
      "Epoch 131/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 48.8382 - mean_squared_error: 42.6562 - val_loss: 29.2247 - val_mean_squared_error: 23.0207\n",
      "Epoch 132/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 48.7630 - mean_squared_error: 42.5663 - val_loss: 24.7651 - val_mean_squared_error: 18.5660\n",
      "Epoch 133/140\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 47.9669 - mean_squared_error: 41.7525 - val_loss: 25.0370 - val_mean_squared_error: 18.8264\n",
      "Epoch 134/140\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 49.4745 - mean_squared_error: 43.2572 - val_loss: 25.1916 - val_mean_squared_error: 18.9678\n",
      "Epoch 135/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 47.1659 - mean_squared_error: 40.9341 - val_loss: 26.0835 - val_mean_squared_error: 19.8420\n",
      "Epoch 136/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 46.5290 - mean_squared_error: 40.2879 - val_loss: 24.5138 - val_mean_squared_error: 18.2715\n",
      "Epoch 137/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 46.6379 - mean_squared_error: 40.3835 - val_loss: 24.5269 - val_mean_squared_error: 18.2704\n",
      "Epoch 138/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 45.6812 - mean_squared_error: 39.4205 - val_loss: 23.2910 - val_mean_squared_error: 17.0260\n",
      "Epoch 139/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 45.0755 - mean_squared_error: 38.8099 - val_loss: 24.7688 - val_mean_squared_error: 18.4853\n",
      "Epoch 140/140\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 44.5563 - mean_squared_error: 38.2734 - val_loss: 24.9487 - val_mean_squared_error: 18.6540\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluate the new model against the test set:\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'gvn': <tf.Tensor 'IteratorGetNext:0' shape=(10,) dtype=bool>, 'instcombine': <tf.Tensor 'IteratorGetNext:2' shape=(10,) dtype=bool>, 'inline': <tf.Tensor 'IteratorGetNext:1' shape=(10,) dtype=bool>, 'jump_threading': <tf.Tensor 'IteratorGetNext:5' shape=(10,) dtype=bool>, 'simplifycfg': <tf.Tensor 'IteratorGetNext:9' shape=(10,) dtype=bool>, 'sccp': <tf.Tensor 'IteratorGetNext:8' shape=(10,) dtype=bool>, 'print_used_types': <tf.Tensor 'IteratorGetNext:7' shape=(10,) dtype=bool>, 'ipsccp': <tf.Tensor 'IteratorGetNext:3' shape=(10,) dtype=bool>, 'iv_users': <tf.Tensor 'IteratorGetNext:4' shape=(10,) dtype=bool>, 'licm': <tf.Tensor 'IteratorGetNext:6' shape=(10,) dtype=bool>}. Consider rewriting this model with the Functional API.\n",
      "41/41 [==============================] - 0s 3ms/step - loss: 45.6032 - mean_squared_error: 39.3085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[45.60315704345703, 39.3084602355957]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.001\n",
    "epochs = 140\n",
    "batch_size = 10\n",
    "\n",
    "label_name = \"PERF\"\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model(learning_rate, feature_layer)\n",
    "\n",
    "# Train the model on the normalized training set.\n",
    "epochs, mse = train_model(my_model, train_df, epochs, \n",
    "                          label_name, batch_size, validation_split=0.1)\n",
    "plot_the_loss_curve(epochs, mse)\n",
    "\n",
    "test_features = {name:np.array(value) for name, value in test_df.items()}\n",
    "test_label = np.array(test_features.pop(label_name)) # isolate the label\n",
    "print(\"\\n Evaluate the new model against the test set:\")\n",
    "my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
