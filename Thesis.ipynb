{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run on TensorFlow 2.x\n",
    "%tensorflow_version 2.x\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import relevant modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "from mlp_sparse_model import MLPSparseModel\n",
    "from mlp_plain_model import MLPPlainModel\n",
    "import time\n",
    "\n",
    "# The following lines adjust the granularity of reporting. \n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.7f}\".format\n",
    "\n",
    "# The following line improves formatting when ouputting NumPy arrays.\n",
    "np.set_printoptions(linewidth = 200)\n",
    "\n",
    "SAMPLE_SIZE = 11\n",
    "N_EXP = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_generator():\n",
    "    # Generate the initial seed for each sample size (to match the seed\n",
    "    # of the results in the paper)\n",
    "    # This is just the initial seed, for each experiment, the seeds will be\n",
    "    # equal the initial seed + the number of the experiment\n",
    "\n",
    "    N_train_all = np.multiply(11, [1, 2, 3, 4, 5])  # This is for LLVM\n",
    "    if SAMPLE_SIZE in N_train_all:\n",
    "        seed_o = np.where(N_train_all == SAMPLE_SIZE)[0][0]\n",
    "    else:\n",
    "        seed_o = np.random.randint(1, 101)\n",
    "\n",
    "    return seed_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Get data\n",
    "data_df = pd.read_csv(\"Data/LLVM_AllNumeric.csv\")\n",
    "column_dict = {name: bool for name in list(data_df.columns.values) if name != 'PERF'}\n",
    "column_dict['PERF'] = \"float64\"\n",
    "data_df = data_df.astype(column_dict)\n",
    "data_df = data_df.reindex(np.random.permutation(data_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gvn</th>\n",
       "      <th>instcombine</th>\n",
       "      <th>inline</th>\n",
       "      <th>jump_threading</th>\n",
       "      <th>simplifycfg</th>\n",
       "      <th>sccp</th>\n",
       "      <th>print_used_types</th>\n",
       "      <th>ipsccp</th>\n",
       "      <th>iv_users</th>\n",
       "      <th>licm</th>\n",
       "      <th>PERF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>211.5866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>212.5166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>238.7833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>247.0133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>250.6133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>229.8166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>228.7166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>237.8900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>234.0533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>257.7133333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>512 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        gvn  instcombine  inline  ...  iv_users   licm        PERF\n",
       "869   False        False    True  ...     False  False 211.5866667\n",
       "747   False        False    True  ...     False  False 212.5166667\n",
       "275    True         True   False  ...      True  False 238.7833333\n",
       "728    True        False    True  ...      True   True 247.0133333\n",
       "1021  False         True    True  ...     False   True 250.6133333\n",
       "...     ...          ...     ...  ...       ...    ...         ...\n",
       "262    True        False   False  ...      True  False 229.8166667\n",
       "269    True        False   False  ...      True  False 228.7166667\n",
       "649    True        False   False  ...     False   True 237.8900000\n",
       "641    True        False   False  ...     False   True 234.0533333\n",
       "578   False        False    True  ...     False   True 257.7133333\n",
       "\n",
       "[512 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gvn</th>\n",
       "      <th>instcombine</th>\n",
       "      <th>inline</th>\n",
       "      <th>jump_threading</th>\n",
       "      <th>simplifycfg</th>\n",
       "      <th>sccp</th>\n",
       "      <th>print_used_types</th>\n",
       "      <th>ipsccp</th>\n",
       "      <th>iv_users</th>\n",
       "      <th>licm</th>\n",
       "      <th>PERF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>247.4266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>250.0666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>240.0733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>236.2266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>234.7733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>251.0900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>234.9200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>253.9633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>249.7866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>231.6300000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>512 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       gvn  instcombine  inline  ...  iv_users   licm        PERF\n",
       "923  False         True    True  ...      True   True 247.4266667\n",
       "894   True         True   False  ...      True   True 250.0666667\n",
       "451   True        False    True  ...     False  False 240.0733333\n",
       "467   True        False    True  ...      True  False 236.2266667\n",
       "737   True        False   False  ...      True   True 234.7733333\n",
       "..     ...          ...     ...  ...       ...    ...         ...\n",
       "718   True        False    True  ...     False   True 251.0900000\n",
       "819  False         True   False  ...     False   True 234.9200000\n",
       "387   True         True    True  ...     False  False 253.9633333\n",
       "922  False         True    True  ...     False   True 249.7866667\n",
       "796  False         True   False  ...      True   True 231.6300000\n",
       "\n",
       "[512 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split data set and set seed\n",
    "seed_init = seed_generator()\n",
    "seed = seed_init*N_EXP + 1\n",
    "np.random.seed(seed_init)\n",
    "train = data_df.sample(frac=0.5)\n",
    "test=data_df.drop(train.index).sample(frac=1.0)\n",
    "display(train)\n",
    "display(test)\n",
    "# train_df, validate_df, test_df = np.split(data_df.sample(frac=1), [int(.5*len(data_df)), int(.9*len(data_df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature layer\n",
    "columns = [column for column in column_dict.keys() if column != 'PERF']\n",
    "feature_columns = []\n",
    "for column in columns:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(column))\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define the plotting function.\n",
    "\n",
    "def plot_the_loss_curve(epochs, mse):\n",
    "  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Mean Squared Error\")\n",
    "\n",
    "  plt.plot(epochs, mse, label=\"Loss\")\n",
    "  plt.legend()\n",
    "  plt.ylim([mse.min()*0.95, mse.max() * 1.03])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Double-click for a possible solution\n",
    "\n",
    "# The following \"solution\" uses L2 regularization to bring training loss\n",
    "# and test loss closer to each other. Many, many other solutions are possible.\n",
    "\n",
    "\n",
    "def create_model(learning_rate, feature_layer):\n",
    "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
    "\n",
    "  # Discard any pre-existing version of the model.\n",
    "  model = None\n",
    "\n",
    "  # Most simple tf.keras models are sequential.\n",
    "  model = tf.keras.models.Sequential()\n",
    "\n",
    "  # Add the layer containing the feature columns to the model.\n",
    "  model.add(feature_layer)\n",
    "\n",
    "  # Describe the topography of the model. \n",
    "\n",
    "  # Implement L2 regularization in the first hidden layer.\n",
    "  model.add(tf.keras.layers.Dense(units=20, \n",
    "                                  activation='relu',\n",
    "                                  # kernel_regularizer=tf.keras.regularizers.l1(0.0001),\n",
    "                                  name='Hidden1'))\n",
    "  \n",
    "  # Implement L2 regularization in the second hidden layer.\n",
    "  model.add(tf.keras.layers.Dense(units=12, \n",
    "                                  activation='relu', \n",
    "                                  # kernel_regularizer=tf.keras.regularizers.l2(0.0001),\n",
    "                                  name='Hidden2'))\n",
    "\n",
    "  # Define the output layer.\n",
    "  model.add(tf.keras.layers.Dense(units=1,  \n",
    "                                  name='Output'))                              \n",
    "  \n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "  return model     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, epochs, label_name,\n",
    "                batch_size=None):\n",
    "  \"\"\"Train the model by feeding it data.\"\"\"\n",
    "\n",
    "  # Split the dataset into features and label.\n",
    "  features = {name:np.array(value) for name, value in dataset.items()}\n",
    "  label = np.array(features.pop(label_name))\n",
    "  history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=True) \n",
    "\n",
    "  # The list of epochs is stored separately from the rest of history.\n",
    "  epochs = history.epoch\n",
    "  \n",
    "  # To track the progression of training, gather a snapshot\n",
    "  # of the model's mean squared error at each epoch. \n",
    "  hist = pd.DataFrame(history.history)\n",
    "  mse = hist[\"mean_squared_error\"]\n",
    "\n",
    "  return epochs, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/140\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'gvn': <tf.Tensor 'IteratorGetNext:0' shape=(None,) dtype=bool>, 'instcombine': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=bool>, 'inline': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=bool>, 'jump_threading': <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=bool>, 'simplifycfg': <tf.Tensor 'IteratorGetNext:9' shape=(None,) dtype=bool>, 'sccp': <tf.Tensor 'IteratorGetNext:8' shape=(None,) dtype=bool>, 'print_used_types': <tf.Tensor 'IteratorGetNext:7' shape=(None,) dtype=bool>, 'ipsccp': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=bool>, 'iv_users': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=bool>, 'licm': <tf.Tensor 'IteratorGetNext:6' shape=(None,) dtype=bool>}. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'gvn': <tf.Tensor 'IteratorGetNext:0' shape=(None,) dtype=bool>, 'instcombine': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=bool>, 'inline': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=bool>, 'jump_threading': <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=bool>, 'simplifycfg': <tf.Tensor 'IteratorGetNext:9' shape=(None,) dtype=bool>, 'sccp': <tf.Tensor 'IteratorGetNext:8' shape=(None,) dtype=bool>, 'print_used_types': <tf.Tensor 'IteratorGetNext:7' shape=(None,) dtype=bool>, 'ipsccp': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=bool>, 'iv_users': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=bool>, 'licm': <tf.Tensor 'IteratorGetNext:6' shape=(None,) dtype=bool>}. Consider rewriting this model with the Functional API.\n",
      "52/52 [==============================] - 1s 2ms/step - loss: 55780.8984 - mean_squared_error: 55779.7852\n",
      "Epoch 2/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 55041.5234 - mean_squared_error: 55040.3867\n",
      "Epoch 3/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 53267.9219 - mean_squared_error: 53266.6367\n",
      "Epoch 4/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 49425.5234 - mean_squared_error: 49424.0078\n",
      "Epoch 5/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 42529.9414 - mean_squared_error: 42527.9961\n",
      "Epoch 6/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 32402.6816 - mean_squared_error: 32400.1582\n",
      "Epoch 7/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 20946.1836 - mean_squared_error: 20942.9707\n",
      "Epoch 8/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 11050.7070 - mean_squared_error: 11046.7764\n",
      "Epoch 9/140\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 5112.7329 - mean_squared_error: 5108.1758\n",
      "Epoch 10/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2996.9741 - mean_squared_error: 2991.9956\n",
      "Epoch 11/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2589.3843 - mean_squared_error: 2584.2405\n",
      "Epoch 12/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2517.1167 - mean_squared_error: 2511.9443\n",
      "Epoch 13/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2480.1504 - mean_squared_error: 2475.0066\n",
      "Epoch 14/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2443.9685 - mean_squared_error: 2438.8350\n",
      "Epoch 15/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2403.8120 - mean_squared_error: 2398.6948\n",
      "Epoch 16/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2363.8354 - mean_squared_error: 2358.7322\n",
      "Epoch 17/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2321.9766 - mean_squared_error: 2316.9160\n",
      "Epoch 18/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2276.4219 - mean_squared_error: 2271.3687\n",
      "Epoch 19/140\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 2226.8516 - mean_squared_error: 2221.8105\n",
      "Epoch 20/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2181.8262 - mean_squared_error: 2176.8079\n",
      "Epoch 21/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2130.3757 - mean_squared_error: 2125.3503\n",
      "Epoch 22/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2078.7708 - mean_squared_error: 2073.7668\n",
      "Epoch 23/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 2026.1874 - mean_squared_error: 2021.1869\n",
      "Epoch 24/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1970.1847 - mean_squared_error: 1965.1880\n",
      "Epoch 25/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1919.3285 - mean_squared_error: 1914.3438\n",
      "Epoch 26/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1860.5095 - mean_squared_error: 1855.5204\n",
      "Epoch 27/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1807.0238 - mean_squared_error: 1802.0322\n",
      "Epoch 28/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1749.9957 - mean_squared_error: 1745.0066\n",
      "Epoch 29/140\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1698.5471 - mean_squared_error: 1693.5907\n",
      "Epoch 30/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1648.5392 - mean_squared_error: 1643.5397\n",
      "Epoch 31/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1581.0251 - mean_squared_error: 1576.0435\n",
      "Epoch 32/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1527.7504 - mean_squared_error: 1522.7615\n",
      "Epoch 33/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1468.6846 - mean_squared_error: 1463.6825\n",
      "Epoch 34/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1411.4390 - mean_squared_error: 1406.4266\n",
      "Epoch 35/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1349.0680 - mean_squared_error: 1344.0464\n",
      "Epoch 36/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1291.1826 - mean_squared_error: 1286.1433\n",
      "Epoch 37/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1226.6425 - mean_squared_error: 1221.5820\n",
      "Epoch 38/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1161.7242 - mean_squared_error: 1156.6392\n",
      "Epoch 39/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1092.2263 - mean_squared_error: 1087.1193\n",
      "Epoch 40/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 1027.9788 - mean_squared_error: 1022.8351\n",
      "Epoch 41/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 960.5211 - mean_squared_error: 955.3549\n",
      "Epoch 42/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 896.5486 - mean_squared_error: 891.3160\n",
      "Epoch 43/140\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 824.9712 - mean_squared_error: 819.7084\n",
      "Epoch 44/140\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 759.5709 - mean_squared_error: 754.2723\n",
      "Epoch 45/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 695.4306 - mean_squared_error: 690.0803\n",
      "Epoch 46/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 634.3300 - mean_squared_error: 628.9291\n",
      "Epoch 47/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 578.4416 - mean_squared_error: 572.9996\n",
      "Epoch 48/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 519.4963 - mean_squared_error: 514.0104\n",
      "Epoch 49/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 467.5511 - mean_squared_error: 462.0153\n",
      "Epoch 50/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 420.8363 - mean_squared_error: 415.2457\n",
      "Epoch 51/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 375.7465 - mean_squared_error: 370.1116\n",
      "Epoch 52/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 334.0403 - mean_squared_error: 328.3543\n",
      "Epoch 53/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 295.8404 - mean_squared_error: 290.1034\n",
      "Epoch 54/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 260.4943 - mean_squared_error: 254.7084\n",
      "Epoch 55/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 230.7566 - mean_squared_error: 224.9153\n",
      "Epoch 56/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 199.7400 - mean_squared_error: 193.8330\n",
      "Epoch 57/140\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 174.5417 - mean_squared_error: 168.5800\n",
      "Epoch 58/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 155.2733 - mean_squared_error: 149.2406\n",
      "Epoch 59/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 133.8791 - mean_squared_error: 127.7940\n",
      "Epoch 60/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 119.6139 - mean_squared_error: 113.4644\n",
      "Epoch 61/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 106.7856 - mean_squared_error: 100.5807\n",
      "Epoch 62/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 98.7013 - mean_squared_error: 92.4497\n",
      "Epoch 63/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 90.8218 - mean_squared_error: 84.5349\n",
      "Epoch 64/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 86.1049 - mean_squared_error: 79.7787\n",
      "Epoch 65/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 82.4586 - mean_squared_error: 76.1034\n",
      "Epoch 66/140\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 80.6562 - mean_squared_error: 74.2744\n",
      "Epoch 67/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 77.0403 - mean_squared_error: 70.6373\n",
      "Epoch 68/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 75.1912 - mean_squared_error: 68.7691\n",
      "Epoch 69/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 74.8675 - mean_squared_error: 68.4328\n",
      "Epoch 70/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 73.7749 - mean_squared_error: 67.3264\n",
      "Epoch 71/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 73.4704 - mean_squared_error: 67.0122\n",
      "Epoch 72/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 73.3326 - mean_squared_error: 66.8665\n",
      "Epoch 73/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 72.8463 - mean_squared_error: 66.3779\n",
      "Epoch 74/140\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 72.5095 - mean_squared_error: 66.0382\n",
      "Epoch 75/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 71.6958 - mean_squared_error: 65.2146\n",
      "Epoch 76/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 72.6625 - mean_squared_error: 66.1825\n",
      "Epoch 77/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 72.1622 - mean_squared_error: 65.6837\n",
      "Epoch 78/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 71.9971 - mean_squared_error: 65.5203\n",
      "Epoch 79/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 74.3338 - mean_squared_error: 67.8598\n",
      "Epoch 80/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 71.4698 - mean_squared_error: 64.9998\n",
      "Epoch 81/140\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 71.7711 - mean_squared_error: 65.3022\n",
      "Epoch 82/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 71.7971 - mean_squared_error: 65.3291\n",
      "Epoch 83/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 71.2710 - mean_squared_error: 64.8082\n",
      "Epoch 84/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 73.2871 - mean_squared_error: 66.8205\n",
      "Epoch 85/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 72.4172 - mean_squared_error: 65.9578\n",
      "Epoch 86/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 71.4415 - mean_squared_error: 64.9839\n",
      "Epoch 87/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 71.6424 - mean_squared_error: 65.1852\n",
      "Epoch 88/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 71.4235 - mean_squared_error: 64.9708\n",
      "Epoch 89/140\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 72.5970 - mean_squared_error: 66.1419\n",
      "Epoch 90/140\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 71.8118 - mean_squared_error: 65.3640\n",
      "Epoch 91/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 71.5037 - mean_squared_error: 65.0562\n",
      "Epoch 92/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 71.4737 - mean_squared_error: 65.0282\n",
      "Epoch 93/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 72.2674 - mean_squared_error: 65.8251\n",
      "Epoch 94/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 70.6128 - mean_squared_error: 64.1728\n",
      "Epoch 95/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 71.3421 - mean_squared_error: 64.9049\n",
      "Epoch 96/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 70.5574 - mean_squared_error: 64.1226\n",
      "Epoch 97/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 72.3298 - mean_squared_error: 65.8951\n",
      "Epoch 98/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 70.1512 - mean_squared_error: 63.7244\n",
      "Epoch 99/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 71.0763 - mean_squared_error: 64.6473\n",
      "Epoch 100/140\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 70.2976 - mean_squared_error: 63.8687\n",
      "Epoch 101/140\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 70.7302 - mean_squared_error: 64.3037\n",
      "Epoch 102/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 70.4106 - mean_squared_error: 63.9846\n",
      "Epoch 103/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 71.4579 - mean_squared_error: 65.0400\n",
      "Epoch 104/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 73.0294 - mean_squared_error: 66.6070\n",
      "Epoch 105/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 69.7467 - mean_squared_error: 63.3368\n",
      "Epoch 106/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 69.8050 - mean_squared_error: 63.3902\n",
      "Epoch 107/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 70.7958 - mean_squared_error: 64.3861\n",
      "Epoch 108/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 69.4444 - mean_squared_error: 63.0403\n",
      "Epoch 109/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 69.5441 - mean_squared_error: 63.1398\n",
      "Epoch 110/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 70.2852 - mean_squared_error: 63.8826\n",
      "Epoch 111/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 69.3286 - mean_squared_error: 62.9235\n",
      "Epoch 112/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 68.8261 - mean_squared_error: 62.4154\n",
      "Epoch 113/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 69.8014 - mean_squared_error: 63.3983\n",
      "Epoch 114/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 70.2845 - mean_squared_error: 63.8720\n",
      "Epoch 115/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 68.3055 - mean_squared_error: 61.8977\n",
      "Epoch 116/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 68.5316 - mean_squared_error: 62.1264\n",
      "Epoch 117/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 69.2199 - mean_squared_error: 62.8080\n",
      "Epoch 118/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 67.5838 - mean_squared_error: 61.1754\n",
      "Epoch 119/140\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 69.1157 - mean_squared_error: 62.7108\n",
      "Epoch 120/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 67.1149 - mean_squared_error: 60.7121\n",
      "Epoch 121/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 66.9615 - mean_squared_error: 60.5674\n",
      "Epoch 122/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 66.3624 - mean_squared_error: 59.9631\n",
      "Epoch 123/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 66.3156 - mean_squared_error: 59.9144\n",
      "Epoch 124/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 68.1891 - mean_squared_error: 61.7869\n",
      "Epoch 125/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 68.1447 - mean_squared_error: 61.7466\n",
      "Epoch 126/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 66.0413 - mean_squared_error: 59.6390\n",
      "Epoch 127/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 66.4723 - mean_squared_error: 60.0748\n",
      "Epoch 128/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 65.3980 - mean_squared_error: 58.9984\n",
      "Epoch 129/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 64.8825 - mean_squared_error: 58.4834\n",
      "Epoch 130/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 65.1026 - mean_squared_error: 58.7099\n",
      "Epoch 131/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 66.3651 - mean_squared_error: 59.9683\n",
      "Epoch 132/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 64.8345 - mean_squared_error: 58.4376\n",
      "Epoch 133/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 64.4553 - mean_squared_error: 58.0647\n",
      "Epoch 134/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 66.1930 - mean_squared_error: 59.7945\n",
      "Epoch 135/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 63.7067 - mean_squared_error: 57.3145\n",
      "Epoch 136/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 63.3934 - mean_squared_error: 57.0031\n",
      "Epoch 137/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 62.5101 - mean_squared_error: 56.1157\n",
      "Epoch 138/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 63.0842 - mean_squared_error: 56.6831\n",
      "Epoch 139/140\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 64.4440 - mean_squared_error: 58.0546\n",
      "Epoch 140/140\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 65.3521 - mean_squared_error: 58.9501\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluate the new model against the test set:\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'gvn': <tf.Tensor 'IteratorGetNext:0' shape=(None,) dtype=bool>, 'instcombine': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=bool>, 'inline': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=bool>, 'jump_threading': <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=bool>, 'simplifycfg': <tf.Tensor 'IteratorGetNext:9' shape=(None,) dtype=bool>, 'sccp': <tf.Tensor 'IteratorGetNext:8' shape=(None,) dtype=bool>, 'print_used_types': <tf.Tensor 'IteratorGetNext:7' shape=(None,) dtype=bool>, 'ipsccp': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=bool>, 'iv_users': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=bool>, 'licm': <tf.Tensor 'IteratorGetNext:6' shape=(None,) dtype=bool>}. Consider rewriting this model with the Functional API.\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 50.7202 - mean_squared_error: 44.3275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[50.720237731933594, 44.327457427978516]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.001\n",
    "epochs = 140\n",
    "batch_size = 10\n",
    "\n",
    "label_name = \"PERF\"\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model(learning_rate, feature_layer)\n",
    "\n",
    "# Train the model on the normalized training set.\n",
    "epochs, mse = train_model(my_model, train_df, epochs, \n",
    "                          label_name, batch_size)\n",
    "plot_the_loss_curve(epochs, mse)\n",
    "\n",
    "test_features = {name:np.array(value) for name, value in test_df.items()}\n",
    "test_label = np.array(test_features.pop(label_name)) # isolate the label\n",
    "print(\"\\n Evaluate the new model against the test set:\")\n",
    "my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
