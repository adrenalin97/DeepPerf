{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run on TensorFlow 2.x\n",
    "%tensorflow_version 2.x\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import relevant modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "from mlp_sparse_model import MLPSparseModel\n",
    "from mlp_plain_model import MLPPlainModel\n",
    "import time\n",
    "\n",
    "# The following lines adjust the granularity of reporting. \n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.7f}\".format\n",
    "\n",
    "# The following line improves formatting when ouputting NumPy arrays.\n",
    "np.set_printoptions(linewidth = 200)\n",
    "\n",
    "SAMPLE_SIZE = 9\n",
    "N_EXP = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_generator():\n",
    "    # Generate the initial seed for each sample size (to match the seed\n",
    "    # of the results in the paper)\n",
    "    # This is just the initial seed, for each experiment, the seeds will be\n",
    "    # equal the initial seed + the number of the experiment\n",
    "\n",
    "    N_train_all = np.multiply(9, [1, 2, 3, 4, 5])  # This is for Apache\n",
    "    if SAMPLE_SIZE in N_train_all:\n",
    "        seed_o = np.where(N_train_all == SAMPLE_SIZE)[0][0]\n",
    "    else:\n",
    "        seed_o = np.random.randint(1, 101)\n",
    "\n",
    "    return seed_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Get data\n",
    "data_df = pd.read_csv(\"Data/Apache_AllNumeric.csv\")\n",
    "column_dict = {name: bool for name in list(data_df.columns.values) if name != 'PERF'}\n",
    "column_dict['PERF'] = \"float64\"\n",
    "data_df = data_df.astype(column_dict)\n",
    "data_df = data_df.reindex(np.random.permutation(data_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize values\n",
    "data_df_mean = data_df.mean()\n",
    "data_df_std = data_df.std()\n",
    "data_df_norm = (data_df - data_df_mean)/data_df_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data set and set seed\n",
    "seed_init = seed_generator()\n",
    "seed = seed_init*N_EXP + 1\n",
    "np.random.seed(seed_init)\n",
    "train_df = data_df_norm.sample(frac=0.6)\n",
    "test_df = data_df_norm.drop(train_df.index).sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature layer\n",
    "columns = [column for column in column_dict.keys() if column != 'PERF']\n",
    "feature_columns = []\n",
    "for column in columns:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(column))\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define the plotting function.\n",
    "\n",
    "def plot_the_loss_curve(epochs, mse):\n",
    "  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Mean Squared Error\")\n",
    "\n",
    "  plt.plot(epochs, mse, label=\"Loss\")\n",
    "  plt.legend()\n",
    "  plt.ylim([mse.min()*0.95, mse.max() * 1.03])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Double-click for a possible solution\n",
    "\n",
    "# The following \"solution\" uses L2 regularization to bring training loss\n",
    "# and test loss closer to each other. Many, many other solutions are possible.\n",
    "\n",
    "\n",
    "def create_model(learning_rate, feature_layer):\n",
    "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
    "\n",
    "  # Discard any pre-existing version of the model.\n",
    "  model = None\n",
    "\n",
    "  # Most simple tf.keras models are sequential.\n",
    "  model = tf.keras.models.Sequential()\n",
    "\n",
    "  # Add the layer containing the feature columns to the model.\n",
    "  model.add(feature_layer)\n",
    "\n",
    "  # Describe the topography of the model. \n",
    "\n",
    "  # Implement L1 regularization in the first hidden layer.\n",
    "  model.add(tf.keras.layers.Dense(units=20, \n",
    "                                  activation='relu',\n",
    "                                  kernel_regularizer=tf.keras.regularizers.l1(0.009),\n",
    "                                  name='Hidden1'))\n",
    "  \n",
    "  # Implement L1 regularization in the second hidden layer.\n",
    "  model.add(tf.keras.layers.Dense(units=12, \n",
    "                                  activation='relu', \n",
    "                                  kernel_regularizer=tf.keras.regularizers.l1(0.009),\n",
    "                                  name='Hidden2'))\n",
    "\n",
    "  # Define the output layer.\n",
    "  model.add(tf.keras.layers.Dense(units=1,  \n",
    "                                  name='Output'))                              \n",
    "  \n",
    "  # input_layer = tf.keras.layers.Input(shape=(None,10))\n",
    "\n",
    "  # layer_1 = tf.keras.layers.Dense(units=20,\n",
    "  #                                 activation='relu',\n",
    "  #                                 kernel_regularizer=tf.keras.regularizers.l1(0.009),\n",
    "  #                                 name='Hidden1')(input_layer)\n",
    "  # layer_2 = tf.keras.layers.Dense(units=12,\n",
    "  #                                 activation='relu',\n",
    "  #                                 kernel_regularizer=tf.keras.regularizers.l1(0.009),\n",
    "  #                                 name='Hidden2')(layer_1)\n",
    "\n",
    "  # output_layer = tf.keras.layers.Dense(units=1,\n",
    "  #                                      name='Output')(layer_2)\n",
    "\n",
    "  # model = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "  return model     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, epochs, label_name,\n",
    "                batch_size=None, validation_split=0.1):\n",
    "  \"\"\"Train the model by feeding it data.\"\"\"\n",
    "\n",
    "  # Split the dataset into features and label.\n",
    "  features = {name:np.array(value) for name, value in dataset.items()}\n",
    "  label = np.array(features.pop(label_name))\n",
    "  history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=True, validation_split=validation_split) \n",
    "\n",
    "  # The list of epochs is stored separately from the rest of history.\n",
    "  epochs = history.epoch\n",
    "  \n",
    "  # To track the progression of training, gather a snapshot\n",
    "  # of the model's mean squared error at each epoch. \n",
    "  hist = pd.DataFrame(history.history)\n",
    "  mse = hist[\"mean_squared_error\"]\n",
    "\n",
    "  return epochs, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'HostnameLookups': <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=float32>, 'KeepAlive': <tf.Tensor 'IteratorGetNext:7' shape=(None,) dtype=float32>, 'EnableSendfile': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=float32>, 'FollowSymLinks': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=float32>, 'AccessLog': <tf.Tensor 'IteratorGetNext:0' shape=(None,) dtype=float32>, 'ExtendedStatus': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=float32>, 'InMemory': <tf.Tensor 'IteratorGetNext:6' shape=(None,) dtype=float32>, 'Handle': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=float32>}. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'HostnameLookups': <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=float32>, 'KeepAlive': <tf.Tensor 'IteratorGetNext:7' shape=(None,) dtype=float32>, 'EnableSendfile': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=float32>, 'FollowSymLinks': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=float32>, 'AccessLog': <tf.Tensor 'IteratorGetNext:0' shape=(None,) dtype=float32>, 'ExtendedStatus': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=float32>, 'InMemory': <tf.Tensor 'IteratorGetNext:6' shape=(None,) dtype=float32>, 'Handle': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=float32>}. Consider rewriting this model with the Functional API.\n",
      " 1/11 [=>............................] - ETA: 7s - loss: 2.2404 - mean_squared_error: 1.4320WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'HostnameLookups': <tf.Tensor 'IteratorGetNext:5' shape=(None,) dtype=float32>, 'KeepAlive': <tf.Tensor 'IteratorGetNext:7' shape=(None,) dtype=float32>, 'EnableSendfile': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=float32>, 'FollowSymLinks': <tf.Tensor 'IteratorGetNext:3' shape=(None,) dtype=float32>, 'AccessLog': <tf.Tensor 'IteratorGetNext:0' shape=(None,) dtype=float32>, 'ExtendedStatus': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=float32>, 'InMemory': <tf.Tensor 'IteratorGetNext:6' shape=(None,) dtype=float32>, 'Handle': <tf.Tensor 'IteratorGetNext:4' shape=(None,) dtype=float32>}. Consider rewriting this model with the Functional API.\n",
      "11/11 [==============================] - 1s 35ms/step - loss: 1.9531 - mean_squared_error: 1.1530 - val_loss: 1.5329 - val_mean_squared_error: 0.7430\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.4263 - mean_squared_error: 0.6437 - val_loss: 1.2755 - val_mean_squared_error: 0.5023\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 1.1498 - mean_squared_error: 0.3831 - val_loss: 1.0939 - val_mean_squared_error: 0.3362\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.9643 - mean_squared_error: 0.2141 - val_loss: 0.9690 - val_mean_squared_error: 0.2290\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.8676 - mean_squared_error: 0.1352 - val_loss: 0.8570 - val_mean_squared_error: 0.1366\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.7960 - mean_squared_error: 0.0863 - val_loss: 0.7704 - val_mean_squared_error: 0.0749\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.7473 - mean_squared_error: 0.0616 - val_loss: 0.7328 - val_mean_squared_error: 0.0605\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.7093 - mean_squared_error: 0.0475 - val_loss: 0.7039 - val_mean_squared_error: 0.0567\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.6731 - mean_squared_error: 0.0362 - val_loss: 0.6688 - val_mean_squared_error: 0.0460\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6422 - mean_squared_error: 0.0300 - val_loss: 0.6394 - val_mean_squared_error: 0.0415\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.6138 - mean_squared_error: 0.0265 - val_loss: 0.6117 - val_mean_squared_error: 0.0390\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5851 - mean_squared_error: 0.0229 - val_loss: 0.5835 - val_mean_squared_error: 0.0351\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.5594 - mean_squared_error: 0.0213 - val_loss: 0.5600 - val_mean_squared_error: 0.0355\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5347 - mean_squared_error: 0.0201 - val_loss: 0.5367 - val_mean_squared_error: 0.0352\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5111 - mean_squared_error: 0.0192 - val_loss: 0.5099 - val_mean_squared_error: 0.0309\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.4880 - mean_squared_error: 0.0181 - val_loss: 0.4885 - val_mean_squared_error: 0.0316\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4652 - mean_squared_error: 0.0175 - val_loss: 0.4670 - val_mean_squared_error: 0.0316\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.4446 - mean_squared_error: 0.0181 - val_loss: 0.4448 - val_mean_squared_error: 0.0302\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4242 - mean_squared_error: 0.0179 - val_loss: 0.4214 - val_mean_squared_error: 0.0271\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4044 - mean_squared_error: 0.0187 - val_loss: 0.4013 - val_mean_squared_error: 0.0270\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.3844 - mean_squared_error: 0.0184 - val_loss: 0.3820 - val_mean_squared_error: 0.0271\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.3658 - mean_squared_error: 0.0189 - val_loss: 0.3617 - val_mean_squared_error: 0.0255\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.3478 - mean_squared_error: 0.0189 - val_loss: 0.3392 - val_mean_squared_error: 0.0210\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.3286 - mean_squared_error: 0.0178 - val_loss: 0.3221 - val_mean_squared_error: 0.0216\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.3112 - mean_squared_error: 0.0180 - val_loss: 0.3064 - val_mean_squared_error: 0.0228\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.2981 - mean_squared_error: 0.0201 - val_loss: 0.2922 - val_mean_squared_error: 0.0227\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.2817 - mean_squared_error: 0.0187 - val_loss: 0.2756 - val_mean_squared_error: 0.0212\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.2678 - mean_squared_error: 0.0193 - val_loss: 0.2624 - val_mean_squared_error: 0.0225\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.2549 - mean_squared_error: 0.0207 - val_loss: 0.2457 - val_mean_squared_error: 0.0187\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.2437 - mean_squared_error: 0.0206 - val_loss: 0.2351 - val_mean_squared_error: 0.0193\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.2320 - mean_squared_error: 0.0217 - val_loss: 0.2273 - val_mean_squared_error: 0.0235\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.2208 - mean_squared_error: 0.0218 - val_loss: 0.2129 - val_mean_squared_error: 0.0208\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.2092 - mean_squared_error: 0.0216 - val_loss: 0.2056 - val_mean_squared_error: 0.0240\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.1996 - mean_squared_error: 0.0222 - val_loss: 0.1968 - val_mean_squared_error: 0.0247\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.1915 - mean_squared_error: 0.0226 - val_loss: 0.1874 - val_mean_squared_error: 0.0239\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.1827 - mean_squared_error: 0.0227 - val_loss: 0.1814 - val_mean_squared_error: 0.0262\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.1754 - mean_squared_error: 0.0233 - val_loss: 0.1738 - val_mean_squared_error: 0.0260\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.1694 - mean_squared_error: 0.0244 - val_loss: 0.1718 - val_mean_squared_error: 0.0303\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.1642 - mean_squared_error: 0.0255 - val_loss: 0.1641 - val_mean_squared_error: 0.0284\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.1599 - mean_squared_error: 0.0265 - val_loss: 0.1520 - val_mean_squared_error: 0.0227\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.1535 - mean_squared_error: 0.0256 - val_loss: 0.1498 - val_mean_squared_error: 0.0252\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.1474 - mean_squared_error: 0.0255 - val_loss: 0.1448 - val_mean_squared_error: 0.0258\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.1426 - mean_squared_error: 0.0260 - val_loss: 0.1384 - val_mean_squared_error: 0.0253\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.1387 - mean_squared_error: 0.0270 - val_loss: 0.1422 - val_mean_squared_error: 0.0328\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.1349 - mean_squared_error: 0.0278 - val_loss: 0.1309 - val_mean_squared_error: 0.0265\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.1305 - mean_squared_error: 0.0266 - val_loss: 0.1312 - val_mean_squared_error: 0.0300\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.1252 - mean_squared_error: 0.0261 - val_loss: 0.1273 - val_mean_squared_error: 0.0307\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.1250 - mean_squared_error: 0.0292 - val_loss: 0.1182 - val_mean_squared_error: 0.0246\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1201 - mean_squared_error: 0.0281 - val_loss: 0.1260 - val_mean_squared_error: 0.0348\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1187 - mean_squared_error: 0.0295 - val_loss: 0.1137 - val_mean_squared_error: 0.0266\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.1125 - mean_squared_error: 0.0261 - val_loss: 0.1137 - val_mean_squared_error: 0.0288\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.1110 - mean_squared_error: 0.0277 - val_loss: 0.1056 - val_mean_squared_error: 0.0237\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.1087 - mean_squared_error: 0.0277 - val_loss: 0.1032 - val_mean_squared_error: 0.0236\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.1049 - mean_squared_error: 0.0270 - val_loss: 0.1031 - val_mean_squared_error: 0.0263\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.1040 - mean_squared_error: 0.0279 - val_loss: 0.0988 - val_mean_squared_error: 0.0245\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.1013 - mean_squared_error: 0.0282 - val_loss: 0.0947 - val_mean_squared_error: 0.0227\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.1001 - mean_squared_error: 0.0282 - val_loss: 0.1020 - val_mean_squared_error: 0.0300\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.0971 - mean_squared_error: 0.0266 - val_loss: 0.0911 - val_mean_squared_error: 0.0226\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.0952 - mean_squared_error: 0.0276 - val_loss: 0.0933 - val_mean_squared_error: 0.0263\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.0928 - mean_squared_error: 0.0263 - val_loss: 0.0886 - val_mean_squared_error: 0.0223\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.0984 - mean_squared_error: 0.0318 - val_loss: 0.1026 - val_mean_squared_error: 0.0357\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.0960 - mean_squared_error: 0.0308 - val_loss: 0.0862 - val_mean_squared_error: 0.0231\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.0932 - mean_squared_error: 0.0303 - val_loss: 0.0826 - val_mean_squared_error: 0.0201\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.0878 - mean_squared_error: 0.0257 - val_loss: 0.0844 - val_mean_squared_error: 0.0236\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.0873 - mean_squared_error: 0.0266 - val_loss: 0.0822 - val_mean_squared_error: 0.0221\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.0846 - mean_squared_error: 0.0251 - val_loss: 0.0796 - val_mean_squared_error: 0.0207\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.0855 - mean_squared_error: 0.0263 - val_loss: 0.0784 - val_mean_squared_error: 0.0194\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.0852 - mean_squared_error: 0.0268 - val_loss: 0.0789 - val_mean_squared_error: 0.0217\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.0838 - mean_squared_error: 0.0264 - val_loss: 0.0790 - val_mean_squared_error: 0.0223\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0828 - mean_squared_error: 0.0264 - val_loss: 0.0776 - val_mean_squared_error: 0.0212\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.0812 - mean_squared_error: 0.0253 - val_loss: 0.0762 - val_mean_squared_error: 0.0206\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 0.0812 - mean_squared_error: 0.0256 - val_loss: 0.0756 - val_mean_squared_error: 0.0202\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0809 - mean_squared_error: 0.0260 - val_loss: 0.0740 - val_mean_squared_error: 0.0199\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.0792 - mean_squared_error: 0.0253 - val_loss: 0.0736 - val_mean_squared_error: 0.0200\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.0788 - mean_squared_error: 0.0254 - val_loss: 0.0772 - val_mean_squared_error: 0.0232\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 0.0796 - mean_squared_error: 0.0262 - val_loss: 0.0734 - val_mean_squared_error: 0.0205\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.0793 - mean_squared_error: 0.0262 - val_loss: 0.0743 - val_mean_squared_error: 0.0209\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.0797 - mean_squared_error: 0.0255 - val_loss: 0.0715 - val_mean_squared_error: 0.0182\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.0777 - mean_squared_error: 0.0252 - val_loss: 0.0709 - val_mean_squared_error: 0.0194\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.0765 - mean_squared_error: 0.0252 - val_loss: 0.0684 - val_mean_squared_error: 0.0172\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.0768 - mean_squared_error: 0.0250 - val_loss: 0.0716 - val_mean_squared_error: 0.0202\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.0767 - mean_squared_error: 0.0257 - val_loss: 0.0675 - val_mean_squared_error: 0.0170\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.0752 - mean_squared_error: 0.0253 - val_loss: 0.0740 - val_mean_squared_error: 0.0239\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.0794 - mean_squared_error: 0.0288 - val_loss: 0.0699 - val_mean_squared_error: 0.0197\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.0753 - mean_squared_error: 0.0254 - val_loss: 0.0686 - val_mean_squared_error: 0.0194\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.0778 - mean_squared_error: 0.0277 - val_loss: 0.0693 - val_mean_squared_error: 0.0192\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.0770 - mean_squared_error: 0.0277 - val_loss: 0.0679 - val_mean_squared_error: 0.0191\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0761 - mean_squared_error: 0.0269 - val_loss: 0.0726 - val_mean_squared_error: 0.0230\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.0746 - mean_squared_error: 0.0255 - val_loss: 0.0678 - val_mean_squared_error: 0.0192\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.0736 - mean_squared_error: 0.0259 - val_loss: 0.0689 - val_mean_squared_error: 0.0219\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.0723 - mean_squared_error: 0.0254 - val_loss: 0.0631 - val_mean_squared_error: 0.0165\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.0706 - mean_squared_error: 0.0244 - val_loss: 0.0638 - val_mean_squared_error: 0.0179\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0708 - mean_squared_error: 0.0249 - val_loss: 0.0643 - val_mean_squared_error: 0.0186\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.0703 - mean_squared_error: 0.0247 - val_loss: 0.0626 - val_mean_squared_error: 0.0172\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.0697 - mean_squared_error: 0.0241 - val_loss: 0.0624 - val_mean_squared_error: 0.0171\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.0692 - mean_squared_error: 0.0244 - val_loss: 0.0609 - val_mean_squared_error: 0.0160\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.0699 - mean_squared_error: 0.0246 - val_loss: 0.0726 - val_mean_squared_error: 0.0265\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.0727 - mean_squared_error: 0.0272 - val_loss: 0.0637 - val_mean_squared_error: 0.0192\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.0735 - mean_squared_error: 0.0288 - val_loss: 0.0621 - val_mean_squared_error: 0.0170\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0705 - mean_squared_error: 0.0256 - val_loss: 0.0647 - val_mean_squared_error: 0.0200\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_features (DenseFeatur  multiple                 0         \n",
      " es)                                                             \n",
      "                                                                 \n",
      " Hidden1 (Dense)             multiple                  180       \n",
      "                                                                 \n",
      " Hidden2 (Dense)             multiple                  252       \n",
      "                                                                 \n",
      " Output (Dense)              multiple                  13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 445\n",
      "Trainable params: 445\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.004\n",
    "epochs = 100\n",
    "batch_size = 10\n",
    "\n",
    "label_name = \"PERF\"\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model(learning_rate, feature_layer)\n",
    "# Train the model on the normalized training set.\n",
    "epochs, mse = train_model(my_model, train_df, epochs, \n",
    "                          label_name, batch_size, validation_split=0.1)\n",
    "my_model.summary()\n",
    "# plot_the_loss_curve(epochs, mse)\n",
    "\n",
    "# test_features = {name:np.array(value) for name, value in test_df.items()}\n",
    "# test_label = np.array(test_features.pop(label_name)) # isolate the label\n",
    "# print(\"\\n Evaluate the new model against the test set:\")\n",
    "# my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size) \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
