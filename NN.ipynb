{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "from mlp_sparse_model import MLPSparseModel\n",
    "from mlp_plain_model import MLPPlainModel\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define params\n",
    "sys_name = 'LLVM'\n",
    "n_exp = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def system_samplesize(sys_name):\n",
    "    if (sys_name == 'Apache'):\n",
    "        N_train_all = np.multiply(9, [1, 2, 3, 4, 5])  # This is for Apache\n",
    "    elif (sys_name == 'BDBJ'):\n",
    "        N_train_all = np.multiply(26, [1, 2, 3, 4, 5])  # This is for BDBJ\n",
    "    elif (sys_name == 'BDBC'):\n",
    "        N_train_all = np.multiply(18, [1, 2, 3, 4, 5])  # This is for BDBC\n",
    "    elif (sys_name == 'LLVM'):\n",
    "        N_train_all = np.multiply(11, [1, 2, 3, 4, 5])  # This is for LLVM\n",
    "    elif (sys_name == 'SQL'):\n",
    "        N_train_all = np.multiply(39, [1, 2, 3, 4, 5])  # This is for SQL\n",
    "    elif (sys_name == 'x264'):\n",
    "        N_train_all = np.multiply(16, [1, 2, 3, 4, 5])  # This is for X264\n",
    "    elif (sys_name == 'Dune'):\n",
    "        N_train_all = np.asarray([49, 78, 240, 375])  # This is for Dune\n",
    "    elif (sys_name == 'hipacc'):\n",
    "        N_train_all = np.asarray([261, 736, 528, 1281])  # This is for hipacc\n",
    "    elif (sys_name == 'hsmgp'):\n",
    "        N_train_all = np.asarray([77, 173, 384, 480])  # This is for hsmgp\n",
    "    elif (sys_name == 'javagc'):\n",
    "        N_train_all = np.asarray([423, 534, 855, 2571])  # This is for javagc\n",
    "    elif (sys_name == 'sac'):\n",
    "        N_train_all = np.asarray([2060, 2295, 2499, 3261])  # This is for sac\n",
    "    else:\n",
    "        raise AssertionError(\"Unexpected value of 'sys_name'!\")\n",
    "\n",
    "    return N_train_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_generator(sys_name, sample_size):\n",
    "    # Generate the initial seed for each sample size (to match the seed\n",
    "    # of the results in the paper)\n",
    "    # This is just the initial seed, for each experiment, the seeds will be\n",
    "    # equal the initial seed + the number of the experiment\n",
    "\n",
    "    N_train_all = system_samplesize(sys_name)\n",
    "    if sample_size in N_train_all:\n",
    "        seed_o = np.where(N_train_all == sample_size)[0][0]\n",
    "    else:\n",
    "        seed_o = np.random.randint(1, 101)\n",
    "\n",
    "    return seed_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_scale_data(X_sample, Y_sample, N_train):\n",
    "    \n",
    "    # Scale X_train and Y_train\n",
    "    max_X = np.amax(X_sample, axis=0)\n",
    "    if 0 in max_X:\n",
    "        max_X[max_X == 0] = 1\n",
    "    X_train = np.divide(X_sample, max_X)\n",
    "    max_Y = np.max(Y_sample)/100\n",
    "    if max_Y == 0:\n",
    "        max_Y = 1\n",
    "    Y_train = np.divide(Y_sample, max_Y)\n",
    "\n",
    "    # Split train data into 2 parts (67-33)\n",
    "    N_cross = int(np.ceil(N_train*2/3))\n",
    "    X_train = X_train[0:N_cross, :]\n",
    "    Y_train = Y_train[0:N_cross]\n",
    "    X_val = X_train[N_cross:N_train, :]\n",
    "    Y_val = Y_train[N_cross:N_train]\n",
    "\n",
    "    return X_train, Y_train, X_val, Y_val, max_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_opt_num_layers(X_train, Y_train, X_val, Y_val, config, errors):\n",
    "        count = 0\n",
    "        layer_range = range(2, 15)\n",
    "        lr_range = np.logspace(np.log10(0.0001), np.log10(0.1), 4)\n",
    "        for n_layer in layer_range:\n",
    "            config['num_layer'] = n_layer\n",
    "            for lr_index, lr_initial in enumerate(lr_range):\n",
    "                # TODO: Use keras for build and train\n",
    "                model = MLPPlainModel(config, \"\")\n",
    "                model.build_train()\n",
    "                model.train(X_train, Y_train, lr_initial)\n",
    "\n",
    "                Y_pred_train = model.predict(X_train)\n",
    "                abs_error_train = np.mean(np.abs(Y_pred_train - Y_train))\n",
    "                errors['abs_error_all_train'][int(n_layer), lr_index] = abs_error_train\n",
    "\n",
    "                Y_pred_val = model.predict(X_val)\n",
    "                abs_error = np.mean(np.abs(Y_pred_val - Y_val))\n",
    "                errors['abs_error_all'][int(n_layer), lr_index] = abs_error\n",
    "\n",
    "            # Pick the learning rate that has the smallest train cost\n",
    "            # Save testing abs_error correspond to the chosen learning_rate\n",
    "            temp = errors['abs_error_all_train'][int(n_layer), :]/np.max(errors['abs_error_all_train'])\n",
    "            temp_idx = np.where(abs(temp) < 0.0001)[0]\n",
    "            if len(temp_idx) > 0:\n",
    "                lr_best = lr_range[np.max(temp_idx)]\n",
    "                err_val_best = errors['abs_error_all'][int(n_layer), np.max(temp_idx)]\n",
    "            else:\n",
    "                lr_best = lr_range[np.argmin(temp)]\n",
    "                err_val_best = errors['abs_error_all'][int(n_layer), np.argmin(temp)]\n",
    "\n",
    "            abs_error_layer_lr[int(n_layer), 0] = err_val_best\n",
    "            abs_error_layer_lr[int(n_layer), 1] = lr_best\n",
    "\n",
    "            if abs_err_layer_lr_min >= errors['abs_error_all'][int(n_layer), np.argmin(temp)]:\n",
    "                abs_err_layer_lr_min = errors['abs_error_all'][int(n_layer), np.argmin(temp)]\n",
    "                count = 0\n",
    "            else:\n",
    "                count += 1\n",
    "\n",
    "            if count >= 2:\n",
    "                break\n",
    "        abs_error_layer_lr = abs_error_layer_lr[abs_error_layer_lr[:, 1] != 0]\n",
    "\n",
    "        # Get the optimal number of layers\n",
    "        n_layer_opt = layer_range[np.argmin(abs_error_layer_lr[:, 0])]+5\n",
    "\n",
    "        return n_layer_opt, n_layer, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_opt_lr(X_train, Y_train, X_val, Y_val, n_break, config, errors):\n",
    "    lr_range = np.logspace(np.log10(0.0001), np.log10(0.1), 4)\n",
    "    for lr_index, lr_initial in enumerate(lr_range):\n",
    "        # TODO: use keras for build and train\n",
    "        model = MLPPlainModel(config, \"\")\n",
    "        model.build_train()\n",
    "        model.train(X_train, Y_train, lr_initial)\n",
    "\n",
    "        Y_pred_train = model.predict(X_train)\n",
    "        abs_error_train = np.mean(np.abs(Y_pred_train - Y_train))\n",
    "        errors['abs_error_all_train'][int(n_break), lr_index] = abs_error_train\n",
    "\n",
    "        Y_pred_val = model.predict(X_val)\n",
    "        abs_error = np.mean(np.abs(Y_pred_val - Y_val))\n",
    "        errors['abs_error_all'][int(n_break), lr_index] = abs_error\n",
    "\n",
    "        temp = errors['abs_error_all_train'][int(n_break), :]/np.max(errors['abs_error_all_train'])\n",
    "        temp_idx = np.where(abs(temp) < 0.0001)[0]\n",
    "        if len(temp_idx) > 0:\n",
    "            lr_best = lr_range[np.max(temp_idx)]\n",
    "        else:\n",
    "            lr_best = lr_range[np.argmin(temp)]\n",
    "\n",
    "        lr_opt = lr_best\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_l1_val(X_train1, Y_train1, X_train2, Y_train2, n_layer, lambd, lr_initial):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X_train1: train input data (2/3 of the whole training data)\n",
    "        Y_train1: train output data (2/3 of the whole training data)\n",
    "        X_train2: validate input data (1/3 of the whole training data)\n",
    "        Y_train2: validate output data (1/3 of the whole training data)\n",
    "        n_layer: number of layers of the neural network\n",
    "        lambd: regularized parameter\n",
    "\n",
    "    \"\"\"\n",
    "    config = dict()\n",
    "    config['num_input'] = X_train1.shape[1]\n",
    "    config['num_layer'] = n_layer\n",
    "    config['num_neuron'] = 128\n",
    "    config['lambda'] = lambd\n",
    "    config['verbose'] = 0\n",
    "\n",
    "    dir_output = 'C:/Users/Downloads/'\n",
    "\n",
    "    # Build and train model\n",
    "    model = MLPSparseModel(config, dir_output)\n",
    "    model.build_train()\n",
    "    model.train(X_train1, Y_train1, lr_initial)\n",
    "\n",
    "    # Evaluate trained model on validation data\n",
    "    Y_pred_val = model.predict(X_train2)\n",
    "    abs_error = np.mean(np.abs(Y_pred_val - Y_train2))\n",
    "    rel_error = np.mean(np.abs(np.divide(Y_train2 - Y_pred_val, Y_train2)))\n",
    "\n",
    "    return abs_error, rel_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_right_lambda():\n",
    "    lambda_range = np.logspace(-2, np.log10(1000), 30)\n",
    "    error_min = np.zeros((1, len(lambda_range)))\n",
    "    rel_error_min = np.zeros((1, len(lambda_range)))\n",
    "    decay = 'NA'\n",
    "    for idx, lambd in enumerate(lambda_range):\n",
    "        val_abserror, val_relerror = nn_l1_val(X_train, Y_train,\n",
    "                                            X_val, Y_val,\n",
    "                                            n_layer_opt, lambd, lr_opt)\n",
    "        error_min[0, idx] = val_abserror\n",
    "        rel_error_min[0, idx] = val_relerror\n",
    "\n",
    "    # Find the value of lambda that minimize error_min\n",
    "    lambda_f = lambda_range[np.argmin(error_min)]\n",
    "\n",
    "    return lambda_f, error_min, rel_error_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_and_test_nn(X_train, Y_train, lr_opt, config):\n",
    "    dir_output = 'C:/Users/Downloads'\n",
    "    model = MLPSparseModel(config, dir_output)\n",
    "    model.build_train()\n",
    "    model.train(X_train, Y_train, lr_opt)\n",
    "\n",
    "    # End measuring time\n",
    "    end = time.time()\n",
    "    time_search_train = end-start\n",
    "    print('Time cost (seconds): {:.2f}'.format(time_search_train))\n",
    "    time_all.append(time_search_train)\n",
    "\n",
    "    # Testing with non-training data (whole data - the training data)\n",
    "    testing_index = np.setdiff1d(np.array(range(N)), training_index)\n",
    "    testing_data = data_df[testing_index, :]\n",
    "    X_test = testing_data[:, 0:n]\n",
    "    X_test = np.divide(X_test, max_X)\n",
    "    Y_test = testing_data[:, n][:, np.newaxis]\n",
    "\n",
    "    Y_pred_test = model.predict(X_test)\n",
    "    Y_pred_test = max_Y*Y_pred_test\n",
    "    rel_error = np.mean(np.abs(np.divide(Y_test.ravel() - Y_pred_test.ravel(), Y_test.ravel())))\n",
    "    rel_error_mean.append(np.mean(rel_error)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-43-2a7df9746039>, line 48)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-43-2a7df9746039>\"\u001b[0;36m, line \u001b[0;32m48\u001b[0m\n\u001b[0;31m    errors = {'abs_error_all': np.zeros((15, 4)), 'abs_error_all_train'_}\u001b[0m\n\u001b[0m                                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    sample_size_all = list(system_samplesize(sys_name))\n",
    "    print('Read whole dataset from csv file ...')\n",
    "    dir_data = 'Data/' + sys_name + '_AllNumeric.csv'\n",
    "    print('Dataset: ' + dir_data)\n",
    "    data_df = pd.read_csv(dir_data)\n",
    "    (N, n) = data_df.shape\n",
    "    \n",
    "    # Some variables to store results\n",
    "    result_sys = []\n",
    "    len_count = 0\n",
    "\n",
    "    for idx in range(len(sample_size_all)):\n",
    "        N_train = sample_size_all[idx]\n",
    "        print(\"Sample size: {}\".format(N_train))\n",
    "        seed_init = seed_generator(sys_name, N_train)\n",
    "\n",
    "        rel_error_mean = []\n",
    "        lambda_all = []\n",
    "        error_min_all = []\n",
    "        rel_error_min_all = []\n",
    "        training_index_all = []\n",
    "        n_layer_all = []\n",
    "        lr_all = []\n",
    "        abs_error_layer_lr_all = []\n",
    "        time_all = []\n",
    "\n",
    "        for m in range(1, n_exp+1):\n",
    "\n",
    "            print(\"Experiment: {}\".format(m))\n",
    "\n",
    "            # Start measure time\n",
    "            start = time.time()\n",
    "\n",
    "            # Set seed and generate training data\n",
    "            seed = seed_init*n_exp + m\n",
    "            np.random.seed(seed)\n",
    "\n",
    "            training_data = data_df.sample(n=N_train)\n",
    "            X_sample = training_data.drop(\"PERF\", axis=1)\n",
    "            Y_sample = training_data[\"PERF\"]\n",
    "\n",
    "            X_train, Y_train, X_val, Y_val = split_and_scale_data(X_sample, Y_sample, N_train, data_df)\n",
    "\n",
    "            # Choosing the right number of hidden layers and , start with 2\n",
    "            # The best layer is when adding more layer and the testing error\n",
    "            # does not increase anymore\n",
    "            print('Tuning hyperparameters for the neural network ...')\n",
    "            print('Step 1: Tuning the number of layers and the learning rate ...')\n",
    "\n",
    "            config = {'num_input':n,\n",
    "                      'num_neuron':128, \n",
    "                      'lambda':'NA', \n",
    "                      'decay':'NA',\n",
    "                      'verbose':0}\n",
    "            dir_output = 'C:/Users/Downloads'\n",
    "            errors = {'abs_error_all': np.zeros((15, 4)), \n",
    "                      'abs_error_all_train': np.zeros((15, 4)), \n",
    "                      'abs_error_layer_lr': np.zeros((15, 2)),\n",
    "                      'abs_err_layer_lr_min': 100}\n",
    "\n",
    "            n_layer_opt, n_break, errors = find_opt_num_layers(X_train, Y_train, X_val, Y_val, config, errors)\n",
    "\n",
    "\n",
    "            # Find the optimal learning rate of the specific layer\n",
    "            config['num_layer'] = n_layer_opt\n",
    "            lr_opt = find_opt_lr(X_train, Y_train, X_val, Y_val, n_break, config, errors)\n",
    "            \n",
    "            print('The optimal number of layers: {}'.format(n_layer_opt))\n",
    "            print('The optimal learning rate: {:.4f}'.format(lr_opt))\n",
    "        \n",
    "            # Use grid search to find the right value of lambda\n",
    "            lambda_f, error_min, rel_error_min = find_right_lambda(X_train, Y_train, X_val, Y_val, n_layer_opt, lr_opt)\n",
    "        \n",
    "            print('Step 2: Tuning the l1 regularized hyperparameter ...')\n",
    "            print('The optimal l1 regularizer: {:.4f}'.format(lambda_f))\n",
    "\n",
    "            # Store some useful results\n",
    "            n_layer_all.append(n_layer_opt)\n",
    "            lr_all.append(lr_opt)\n",
    "            abs_error_layer_lr_all.append(errors['abs_error_layer_lr'])\n",
    "            lambda_all.append(lambda_f)\n",
    "            error_min_all.append(error_min)\n",
    "            rel_error_min_all.append(rel_error_min)\n",
    "\n",
    "            # Solve the final NN with the chosen lambda_f on the training data\n",
    "            config = dict()\n",
    "            config['num_neuron'] = 128\n",
    "            config['num_input'] = n\n",
    "            config['num_layer'] = n_layer_opt\n",
    "            config['lambda'] = lambda_f\n",
    "            config['verbose'] = 1\n",
    "\n",
    "            rel_error = solve_and_test_nn(config)\n",
    "\n",
    "            print('Prediction relative error (%): {:.2f}'.format(np.mean(rel_error)*100))\n",
    "\n",
    "        result = dict()\n",
    "        result[\"N_train\"] = N_train\n",
    "        result[\"lambda_all\"] = lambda_all\n",
    "        result[\"n_layer_all\"] = n_layer_all\n",
    "        result[\"lr_all\"] = lr_all\n",
    "        result[\"abs_error_layer_lr_all\"] = abs_error_layer_lr_all\n",
    "        result[\"rel_error_mean\"] = rel_error_mean\n",
    "        result[\"dir_data\"] = dir_data\n",
    "        result[\"error_min_all\"] = error_min_all\n",
    "        result[\"rel_error_min_all\"] = rel_error_min_all\n",
    "        result[\"training_index\"] = training_index_all\n",
    "        result[\"time_search_train\"] = time_all\n",
    "        result_sys.append(result)\n",
    "\n",
    "        # Compute some statistics: mean, confidence interval\n",
    "        result = []\n",
    "        for i in range(len(result_sys)):\n",
    "            temp = result_sys[i]\n",
    "            sd_error_temp = np.sqrt(np.var(temp['rel_error_mean'], ddof=1))\n",
    "            ci_temp = 1.96*sd_error_temp/np.sqrt(len(temp['rel_error_mean']))\n",
    "\n",
    "            result_exp = [temp['N_train'], np.mean(temp['rel_error_mean']),\n",
    "                          ci_temp]\n",
    "            result.append(result_exp)\n",
    "\n",
    "        result_arr = np.asarray(result)\n",
    "\n",
    "        print('Finish experimenting for system {} with sample size {}.'.format(sys_name, N_train))\n",
    "\n",
    "        print('Mean prediction relative error (%) is: {:.2f}, Margin (%) is: {:.2f}'.format(np.mean(rel_error_mean), ci_temp))        \n",
    "\n",
    "        # Save the result statistics to a csv file after each sample\n",
    "        # Save the raw results to an .npy file\n",
    "        print('Save results to the current directory ...')\n",
    "\n",
    "        filename = 'result_' + sys_name + '.csv'\n",
    "        np.savetxt(filename, result_arr, fmt=\"%f\", delimiter=\",\",\n",
    "                   header=\"Sample size, Mean, Margin\")\n",
    "        print('Save the statistics to file ' + filename + ' ...')\n",
    "\n",
    "        filename = 'result_' + sys_name + '_AutoML_veryrandom.npy'\n",
    "        np.save(filename, result_sys)\n",
    "        print('Save the raw results to file ' + filename + ' ...')\n",
    "\n",
    "\n",
    "##Plot the performance predictions\n",
    "#plt.figure()\n",
    "#plt.plot(Y_test, 'r')\n",
    "#plt.plot(Y_pred_test, 'b')\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "#plt.figure()\n",
    "#plt.plot(lambda_range, rel_error_min[0])\n",
    "#plt.plot(error_min[0])\n",
    "#\n",
    "#\n",
    "#plt.figure()\n",
    "#plt.plot(rel_error_min_all[0][0])\n",
    "\n",
    "\n",
    "## Load the raw result and compute the statistics \n",
    "## Compute the statistics (mean and confidence interval)\n",
    "#result_temp = np.load('result_Apache_AutoML_veryrandom.npy').tolist()\n",
    "#for idx in range(5):\n",
    "#    rel_error_mean_temp = result_temp[idx]['rel_error_mean']\n",
    "#    N_train_temp = result_temp[idx]['N_train']\n",
    "#    print(N_train_temp)\n",
    "#    print(np.mean(rel_error_mean_temp))\n",
    "#\n",
    "#    sd_error_temp = np.sqrt(np.var(rel_error_mean_temp, ddof=1))\n",
    "#    ci_temp = 1.96*sd_error_temp/np.sqrt(len(rel_error_mean_temp))\n",
    "#\n",
    "#    print(ci_temp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read whole dataset from csv file ...\n",
      "Dataset: Data/LLVM_AllNumeric.csv\n",
      "Sample size: 11\n",
      "Experiment: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(feature_range=     gvn  instcombine  inline  ...  iv_users  licm        PERF\n",
       "830    0            1       0  ...         1     1  235.780000\n",
       "795    0            1       0  ...         0     1  235.853333\n",
       "495    0            0       0  ...         1     1  214.370000\n",
       "822    0            1       0  ...         0     1  236.736667\n",
       "859    1            1       0  ...         1     1  247.803333\n",
       "104    0            1       1  ...         1     0  226.130000\n",
       "27     1            1       1  ...         0     1  252.446667\n",
       "986    0            1       1  ...         0     1  249.493333\n",
       "735    1            0       0  ...         0     1  236.640000\n",
       "786    0            1       0  ...         0     1  237.103333\n",
       "491    0            0       0  ...         1     1  216.586667\n",
       "\n",
       "[11 rows x 11 columns])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gvn</th>\n",
       "      <th>instcombine</th>\n",
       "      <th>inline</th>\n",
       "      <th>jump_threading</th>\n",
       "      <th>simplifycfg</th>\n",
       "      <th>sccp</th>\n",
       "      <th>print_used_types</th>\n",
       "      <th>ipsccp</th>\n",
       "      <th>iv_users</th>\n",
       "      <th>licm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     gvn  instcombine  inline  ...  ipsccp  iv_users  licm\n",
       "830  0.0          1.0     0.0  ...     1.0       1.0   1.0\n",
       "795  0.0          1.0     0.0  ...     0.0       0.0   1.0\n",
       "495  0.0          0.0     0.0  ...     0.0       1.0   1.0\n",
       "822  0.0          1.0     0.0  ...     0.0       0.0   1.0\n",
       "859  1.0          1.0     0.0  ...     0.0       1.0   1.0\n",
       "104  0.0          1.0     1.0  ...     0.0       1.0   0.0\n",
       "27   1.0          1.0     1.0  ...     1.0       0.0   1.0\n",
       "986  0.0          1.0     1.0  ...     0.0       0.0   1.0\n",
       "735  1.0          0.0     0.0  ...     1.0       0.0   1.0\n",
       "786  0.0          1.0     0.0  ...     0.0       0.0   1.0\n",
       "491  0.0          0.0     0.0  ...     0.0       1.0   1.0\n",
       "\n",
       "[11 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "830     93.397945\n",
       "795     93.426994\n",
       "495     84.916946\n",
       "822     93.776903\n",
       "859     98.160668\n",
       "104     89.575356\n",
       "27     100.000000\n",
       "986     98.830116\n",
       "735     93.738611\n",
       "786     93.922149\n",
       "491     85.795019\n",
       "Name: PERF, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-b46715ff3e0a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Experiment: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mX_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_and_scale_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m# Choosing the right number of hidden layers and , start with 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
